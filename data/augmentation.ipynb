{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, random\n",
    "from imblearn.over_sampling import SMOTE, KMeansSMOTE, SVMSMOTE, BorderlineSMOTE, ADASYN\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "OVERSAMPLE = False\n",
    "AUGMENT = False\n",
    "AUGMENT_NEOPLASIA = False\n",
    "DO_SMOTE = True\n",
    "DO_SMOTE_SVM = True\n",
    "DO_SMOTE_KMEAN = True\n",
    "DO_SMOTE_BORDER = True\n",
    "DO_SMOTE_ADASYN = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation\n",
    "\n",
    "Augment dataset by generating new neoplasia samples by averaging random neoplasia samples. This is better than scaling records (think of the effect on the spectra) imo - but this can be tested!\n",
    "\n",
    "I am augmenting using the training set, to avoid information leakage\n",
    "\n",
    "I am only uagmenting neoplasia, since looking at evaluation metrics and the spectra, squamous tissue is already successfuly / easily identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('original_data/noExclusion_train_data.csv', header=None).reset_index(drop=True)\n",
    "label = pd.read_csv('original_data/noExclusion_train_label.csv', header=None, names=['label']).astype(int)\n",
    "combined = pd.merge(data, label, left_index=True, right_index=True)\n",
    "\n",
    "# print(combined['label'].value_counts())\n",
    "# 1: 137, 2: 257, 3: 178\n",
    "squamous = combined.loc[combined['label']==1].reset_index(drop=True)\n",
    "ndbe = combined.loc[combined['label']==2].reset_index(drop=True)\n",
    "neoplasia = combined.loc[combined['label']==3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment dataset (balance NDBE and neoplasia)\n",
    "\n",
    "if OVERSAMPLE:\n",
    "    # need to make 79 new neoplasia records\n",
    "    for i in range(79):\n",
    "        rand1 = random.randint(0,178) # choose any of the neoplasia records\n",
    "        rand2 = random.randint(0,178)\n",
    "        records = neoplasia.iloc[[rand1,rand2]]\n",
    "        # average the 2 records to make a new neoplasia record\n",
    "        new_neoplasia = records.mean().to_frame().transpose()\n",
    "        neoplasia = pd.concat([neoplasia, new_neoplasia], axis=0)\n",
    "\n",
    "    neoplasia.reset_index(drop=True)\n",
    "    neoplasia['label'] = neoplasia['label'].astype(int)\n",
    "\n",
    "    augmented = pd.concat([squamous, ndbe, neoplasia]).reset_index(drop=True)\n",
    "\n",
    "    x_train = augmented.drop('label', axis=1)\n",
    "    y_train = augmented['label']\n",
    "\n",
    "    x_train.to_csv('augmented_data/train_data.csv', index=False, header=False)\n",
    "    y_train.to_csv('augmented_data/train_label.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment dataset squamous to 150, NDBE and neoplasia to 300\n",
    "# why? To create more training, whilst not having too much fake data\n",
    "# any more tha 300 and neoplasia wouldve had more fake data than real\n",
    "# 300 for NDBE to match neoplasia and balance it, since they have similar spectra\n",
    "# 150 to squamous so it isn't too underrepresented - although it doesn't really struggle\n",
    "# since it is so distinct from NDBE and neoplasia\n",
    "\n",
    "if AUGMENT:\n",
    "    # need to make 122 new neoplasia records\n",
    "    for i in range(122):\n",
    "        rand1 = random.randint(0,178) # choose any of the neoplasia records\n",
    "        rand2 = random.randint(0,178)\n",
    "        records = neoplasia.iloc[[rand1,rand2]]\n",
    "        # average the 2 records to make a new neoplasia record\n",
    "        new_neoplasia = records.mean().to_frame().transpose()\n",
    "        neoplasia = pd.concat([neoplasia, new_neoplasia], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # need to make 43 new ndbe records\n",
    "    for i in range(43):\n",
    "        rand1 = random.randint(0,257) # choose any of the ndbe records\n",
    "        rand2 = random.randint(0,257)\n",
    "        records = ndbe.iloc[[rand1,rand2]]\n",
    "        # average the 2 records to make a new ndbe record\n",
    "        new_ndbe = records.mean().to_frame().transpose()\n",
    "        ndbe = pd.concat([ndbe, new_ndbe], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # need to make 13 new squamous records\n",
    "    for i in range(13):\n",
    "        rand1 = random.randint(0,137) # choose any of the ndbe records\n",
    "        rand2 = random.randint(0,137)\n",
    "        records = squamous.iloc[[rand1,rand2]]\n",
    "        # average the 2 records to make a new ndbe record\n",
    "        new_squamous = records.mean().to_frame().transpose()\n",
    "        squamous = pd.concat([squamous, new_squamous], axis=0).reset_index(drop=True)\n",
    "\n",
    "    augmented = pd.concat([squamous, ndbe, neoplasia]).reset_index(drop=True)\n",
    "\n",
    "    x_train = augmented.drop('label', axis=1)\n",
    "    y_train = augmented['label'].astype(int)\n",
    "\n",
    "    x_train.to_csv('augmented_datav2/train_data.csv', index=False, header=False)\n",
    "    y_train.to_csv('augmented_datav2/train_label.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment dataset (neoplasia only) and create an equal number of augmented data as\n",
    "# real data - see if it improves recall\n",
    "\n",
    "if AUGMENT_NEOPLASIA:\n",
    "    # need to make 178 new neoplasia records\n",
    "    for i in range(178):\n",
    "        rand1 = random.randint(0,178) # choose any of the neoplasia records\n",
    "        rand2 = random.randint(0,178)\n",
    "        records = neoplasia.iloc[[rand1,rand2]]\n",
    "        # average the 2 records to make a new neoplasia record\n",
    "        new_neoplasia = records.mean().to_frame().transpose()\n",
    "        neoplasia = pd.concat([neoplasia, new_neoplasia], axis=0).reset_index(drop=True)\n",
    "\n",
    "    augmented = pd.concat([squamous, ndbe, neoplasia]).reset_index(drop=True)\n",
    "\n",
    "    x_train = augmented.drop('label', axis=1)\n",
    "    y_train = augmented['label'].astype(int)\n",
    "\n",
    "    x_train.to_csv('augmented_datav3/train_data.csv', index=False, header=False)\n",
    "    y_train.to_csv('augmented_datav3/train_label.csv', index=False, header=False)\n",
    "\n",
    "    augmented['label'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre smote: label\n",
      "2        257\n",
      "3        178\n",
      "1        137\n",
      "dtype: int64\n",
      "post smote: label\n",
      "1        257\n",
      "2        257\n",
      "3        257\n",
      "dtype: int64\n",
      "pre kmean smote: label\n",
      "2        257\n",
      "3        178\n",
      "1        137\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post kmean smote: label\n",
      "3        260\n",
      "1        258\n",
      "2        257\n",
      "dtype: int64\n",
      "pre svm smote: label\n",
      "2        257\n",
      "3        178\n",
      "1        137\n",
      "dtype: int64\n",
      "post svm smote: label\n",
      "1        257\n",
      "2        257\n",
      "3        257\n",
      "dtype: int64\n",
      "pre border smote: label\n",
      "2        257\n",
      "3        178\n",
      "1        137\n",
      "dtype: int64\n",
      "post border smote: label\n",
      "1        257\n",
      "2        257\n",
      "3        257\n",
      "dtype: int64\n",
      "pre adasyn smote: label\n",
      "2        257\n",
      "3        178\n",
      "1        137\n",
      "dtype: int64\n",
      "post adasyn smote: label\n",
      "1        257\n",
      "2        257\n",
      "3        253\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# (Imblearn, 2014), (Brownlee, 2020)\n",
    "# trying every smote technique that isn't for categorical \n",
    "# print(data.shape)\n",
    "if DO_SMOTE:\n",
    "    smote = SMOTE(random_state=1)\n",
    "    print(\"pre smote:\", label.value_counts())\n",
    "    x, y = smote.fit_resample(data, label)\n",
    "    print(f\"post smote: {y.value_counts()}\")\n",
    "    x.to_csv('SMOTE/smote_train_data.csv', index=False, header=False)\n",
    "    y.to_csv('SMOTE/smote_train_label.csv', index=False, header=False)\n",
    "\n",
    "# print(data.shape)\n",
    "    \n",
    "if DO_SMOTE_KMEAN:\n",
    "    smote = KMeansSMOTE(random_state=1)\n",
    "    print(\"pre kmean smote:\", label.value_counts())\n",
    "    x, y = smote.fit_resample(data, label)\n",
    "    print(f\"post kmean smote: {y.value_counts()}\")\n",
    "    x.to_csv('SMOTE/kmeanssmote_train_data.csv', index=False, header=False)\n",
    "    y.to_csv('SMOTE/kmeanssmote_train_label.csv', index=False, header=False)\n",
    "\n",
    "if DO_SMOTE_SVM:\n",
    "    smote = SVMSMOTE(random_state=1)\n",
    "    print(\"pre svm smote:\", label.value_counts())\n",
    "    x, y = smote.fit_resample(data, label)\n",
    "    print(f\"post svm smote: {y.value_counts()}\")\n",
    "    x.to_csv('SMOTE/svmsmote_train_data.csv', index=False, header=False)\n",
    "    y.to_csv('SMOTE/svmsmote_train_label.csv', index=False, header=False)\n",
    "\n",
    "if DO_SMOTE_BORDER:\n",
    "    smote = BorderlineSMOTE(random_state=1)\n",
    "    print(\"pre border smote:\", label.value_counts())\n",
    "    x, y = smote.fit_resample(data, label)\n",
    "    print(f\"post border smote: {y.value_counts()}\")\n",
    "    x.to_csv('SMOTE/bordersmote_train_data.csv', index=False, header=False)\n",
    "    y.to_csv('SMOTE/bordersmote_train_label.csv', index=False, header=False)\n",
    "    \n",
    "if DO_SMOTE_ADASYN:\n",
    "    smote = ADASYN(random_state=1)\n",
    "    print(\"pre adasyn smote:\", label.value_counts())\n",
    "    x, y = smote.fit_resample(data, label)\n",
    "    print(f\"post adasyn smote: {y.value_counts()}\")\n",
    "    x.to_csv('SMOTE/adasynsmote_train_data.csv', index=False, header=False)\n",
    "    y.to_csv('SMOTE/adasynsmote_train_label.csv', index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
