{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import os, json, itertools\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score, confusion_matrix, accuracy_score, f1_score, precision_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from skopt import BayesSearchCV, Optimizer\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ensembles = {}\n",
    "done_experiments = {\n",
    "               'snv_svmsmote':{'SNV':True, 'SVMSMOTE':True}, \n",
    "               'augmentedv3':{'AUGMENTEDv3':True}, \n",
    "               'augmentedv3_FS':{'AUGMENTEDv3':True, 'FEATURE_SELECT':True}, \n",
    "               'snv_augmentedv3':{'SNV':True, 'AUGMENTEDv3':True}, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bordersmote\n",
      "no feature selection\n",
      "no FSv2\n",
      "no SNV standardisation\n",
      "params = {'max_depth': None, 'max_features': 'sqrt'} \n",
      "best_score: 0.8457542457542457\n",
      "params = {'var_smoothing': 1e-20} \n",
      "best_score: 0.6562437562437562\n",
      "params = {'n_neighbors': 1} \n",
      "best_score: 0.9040792540792539\n",
      "params = {'C': 5000, 'gamma': 'scale'} \n",
      "best_score: 0.9144688644688644\n",
      "params = {'C': 1000, 'gamma': 'scale'} \n",
      "best_score: 0.8107392607392608\n",
      "params = {'C': 0.1, 'gamma': 'auto'} \n",
      "best_score: 0.3992673992673993\n",
      "params = {'max_depth': 5, 'n_estimators': 100} \n",
      "best_score: 0.9028471528471528\n",
      "params = {'learning_rate': 1, 'n_estimators': 500} \n",
      "best_score: 0.6770895770895772\n",
      "params = {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'} \n",
      "best_score: 0.7757242757242757\n",
      "params = {'max_depth': 10, 'max_features': 5, 'n_estimators': 50} \n",
      "best_score: 0.9093073593073593\n",
      "augmented\n",
      "no FSv2\n",
      "no SNV standardisation\n",
      "params = {'max_depth': 10, 'max_features': 'sqrt'} \n",
      "best_score: 0.7942657342657343\n",
      "params = {'var_smoothing': 1e-20} \n",
      "best_score: 0.6420979020979021\n",
      "params = {'n_neighbors': 4} \n",
      "best_score: 0.8234731934731935\n",
      "params = {'C': 5000, 'gamma': 'scale'} \n",
      "best_score: 0.8541724941724942\n",
      "params = {'C': 100, 'gamma': 'scale'} \n",
      "best_score: 0.7742890442890442\n",
      "params = {'C': 100, 'gamma': 'auto'} \n",
      "best_score: 0.4684382284382285\n",
      "params = {'max_depth': 10, 'n_estimators': 1000} \n",
      "best_score: 0.804941724941725\n",
      "params = {'learning_rate': 0.01, 'n_estimators': 50} \n",
      "best_score: 0.7374125874125874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'} \n",
      "best_score: 0.7712587412587413\n",
      "params = {'max_depth': None, 'max_features': 1, 'n_estimators': 50} \n",
      "best_score: 0.8295104895104896\n",
      "kmeanssmote\n",
      "no feature selection\n",
      "no FSv2\n",
      "no SNV standardisation\n",
      "params = {'max_depth': 10, 'max_features': 'sqrt'} \n",
      "best_score: 0.8544122544122544\n",
      "params = {'var_smoothing': 1e-20} \n",
      "best_score: 0.7214119214119215\n",
      "params = {'n_neighbors': 1} \n",
      "best_score: 0.9137029637029637\n",
      "params = {'C': 5000, 'gamma': 'scale'} \n",
      "best_score: 0.9059440559440558\n",
      "params = {'C': 10, 'gamma': 'scale'} \n",
      "best_score: 0.8065934065934066\n",
      "params = {'C': 100, 'gamma': 'auto'} \n",
      "best_score: 0.6646020646020647\n",
      "params = {'max_depth': 10, 'n_estimators': 500} \n",
      "best_score: 0.8957042957042957\n",
      "params = {'learning_rate': 0.1, 'n_estimators': 10} \n",
      "best_score: 0.7758574758574758\n",
      "params = {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'} \n",
      "best_score: 0.7963203463203462\n",
      "params = {'max_depth': None, 'max_features': 5, 'n_estimators': 100} \n",
      "best_score: 0.9046953046953046\n"
     ]
    }
   ],
   "source": [
    "# for experiment in experiments, set pipeline flags and run bayesian optimisation on each model\n",
    "# record results in new scoreboard - are there any improvements?\n",
    "# if yes, use these models for ensemble, otherwise use gridsearch acquired models for ensemble\n",
    "\n",
    "# to dynamically toggle flags for each experiment\n",
    "# all of the experiments that were grid and bayes searched\n",
    "experiments = {\n",
    "               'bordersmote':{'BORDERSMOTE':True}, \n",
    "               'augmented_FS':{'AUGMENTED':True, 'FEATURE_SELECT':True}, \n",
    "               'kmeanssmote':{'KMEANSSMOTE':True}, \n",
    "               'svmsmote':{'SVMSMOTE':True}, \n",
    "               'smote':{'SMOTE':True}, \n",
    "               'adasynsmote':{'ADASYNSMOTE':True}, \n",
    "               'snv_FS_balanced':{'SNV':True, 'FEATURE_SELECT':True, 'BALANCED':True}, \n",
    "               'raw':{}, \n",
    "               'feature_selectv2':{'FEATURE_SELECTv2':True}\n",
    "}\n",
    "\n",
    "\n",
    "for experiment in experiments:\n",
    "    # accuracies = []\n",
    "    SNV = False\n",
    "    FEATURE_SELECT = False\n",
    "    FEATURE_SELECTv2 = False\n",
    "    BALANCED = False\n",
    "    MY_BALANCED = False\n",
    "    MY_BALANCEDv2 = False\n",
    "    AUGMENTED = False # oversampling of neoplasia\n",
    "    AUGMENTEDv2 = False # actual augmented\n",
    "    AUGMENTEDv3 = False # augmented neoplasia only\n",
    "    SMOTE = False\n",
    "    SVMSMOTE = False\n",
    "    KMEANSSMOTE = False\n",
    "    ADASYNSMOTE = False\n",
    "    BORDERSMOTE = False\n",
    "    \n",
    "    # set flags\n",
    "    try:\n",
    "        SNV = experiments[experiment]['SNV']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'SNV not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        FEATURE_SELECT = experiments[experiment]['FEATURE_SELECT']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'FEATURE_SELECT not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        FEATURE_SELECTv2 = experiments[experiment]['FEATURE_SELECTv2']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'FEATURE_SELECTv2 not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        BALANCED = experiments[experiment]['BALANCED']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'BALANCED not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        AUGMENTED = experiments[experiment]['AUGMENTED']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'AUGMENTED not in {experiment}')\n",
    "    \n",
    "    try:\n",
    "        MY_BALANCEDv2 = experiments[experiment]['MY_BALANCEDv2']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'MY_BALANCEDv2 not in {experiment}')\n",
    "    \n",
    "    try:\n",
    "        AUGMENTEDv2 = experiments[experiment]['AUGMENTEDv2']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'AUGMENTEDv2 not in {experiment}')\n",
    "    \n",
    "    try:\n",
    "        AUGMENTEDv3 = experiments[experiment]['AUGMENTEDv3']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'AUGMENTEDv3 not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        SMOTE = experiments[experiment]['SMOTE']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'SMOTE not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        SVMSMOTE = experiments[experiment]['SVMSMOTE']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'SVMSMOTE not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        KMEANSSMOTE = experiments[experiment]['KMEANSSMOTE']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'KMEANSSMOTE not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        ADASYNSMOTE = experiments[experiment]['ADASYNSMOTE']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'ADASYNSMOTE not in {experiment}')\n",
    "\n",
    "    try:\n",
    "        BORDERSMOTE = experiments[experiment]['BORDERSMOTE']\n",
    "    except:\n",
    "        pass\n",
    "        # print(f'BORDERSMOTE not in {experiment}')\n",
    "\n",
    "    # Choose dataset\n",
    "    DATASET = 'raw'\n",
    "    FILENAME_ACC = 'EnsembleMetrics/raw_dataset/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'EnsembleMetrics/raw_dataset/model_metrics_recall_ensemble.json'\n",
    "\n",
    "    if BALANCED:\n",
    "        DATASET = 'balanced'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/balanced_dataset/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/balanced_dataset/model_metrics_recall_ensemble.json'\n",
    "        if SNV:\n",
    "            DATASET = 'snv_balanced'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/balanced_dataset/model_metrics_accuracy_ensemble_snv.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/balanced_dataset/model_metrics_recall_ensemble_snv.json'\n",
    "            if FEATURE_SELECT:\n",
    "                DATASET = 'snv_FS_balanced'\n",
    "                FILENAME_ACC = 'EnsembleMetrics/balanced_dataset/model_metrics_accuracy_ensemble_snv_FS.json'\n",
    "                FILENAME_RECALL = 'EnsembleMetrics/balanced_dataset/model_metrics_recall_ensemble_snv_FS.json'\n",
    "    elif MY_BALANCED:\n",
    "        DATASET = 'my_balanced'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/my_balanced_dataset/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/my_balanced_dataset/model_metrics_recall_ensemble.json'\n",
    "        if FEATURE_SELECT:\n",
    "            DATASET = 'FS_my_balanced'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/my_balanced_dataset/model_metrics_accuracy_ensemble_FS.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/my_balanced_dataset/model_metrics_recall_ensemble_FS.json'\n",
    "            if SNV:\n",
    "                DATASET = 'snv_FS_my_balanced'\n",
    "                FILENAME_ACC = 'EnsembleMetrics/my_balanced_dataset/model_metrics_accuracy_ensemble_snv_FS.json'\n",
    "                FILENAME_RECALL = 'EnsembleMetrics/my_balanced_dataset/model_metrics_recall_ensemble_snv_FS.json'\n",
    "        elif FEATURE_SELECTv2:\n",
    "            DATASET = 'FSv2_my_balanced'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/my_balanced_dataset/model_metrics_accuracy_ensemble_FSv2.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/my_balanced_dataset/model_metrics_recall_ensemble_FSv2.json'\n",
    "        elif SNV:\n",
    "            DATASET = 'snv_my_balanced'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/my_balanced_dataset/model_metrics_accuracy_ensemble_snv.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/my_balanced_dataset/model_metrics_recall_ensemble_snv.json'\n",
    "    elif AUGMENTED:\n",
    "        DATASET = 'augmented'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/augmented_dataset/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/augmented_dataset/model_metrics_recall_ensemble.json'\n",
    "        if FEATURE_SELECT:\n",
    "            DATASET = 'augmented_FS'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/augmented_dataset/model_metrics_accuracy_ensemble_FS.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/augmented_dataset/model_metrics_recall_ensemble_FS.json'\n",
    "    elif AUGMENTEDv2:\n",
    "        DATASET = 'augmentedv2'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/augmentedv2_dataset/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/augmentedv2_dataset/model_metrics_recall_ensemble.json'\n",
    "        if FEATURE_SELECT:\n",
    "            DATASET = 'augmentedv2_FS'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/augmentedv2_dataset/model_metrics_accuracy_ensemble_FS.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/augmentedv2_dataset/model_metrics_recall_ensemble_FS.json'\n",
    "    elif AUGMENTEDv3:\n",
    "        DATASET = 'augmentedv3'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/augmentedv3_dataset/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/augmentedv3_dataset/model_metrics_recall_ensemble.json'\n",
    "        if SNV:\n",
    "            DATASET = 'snv_augmentedv3'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/augmentedv3_dataset/model_metrics_accuracy_ensemble_snv.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/augmentedv3_dataset/model_metrics_recall_ensemble_snv.json'\n",
    "        if FEATURE_SELECT:\n",
    "            DATASET = 'augmentedv3_FS'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/augmentedv3_dataset/model_metrics_accuracy_ensemble_FS.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/augmentedv3_dataset/model_metrics_recall_ensemble_FS.json'\n",
    "    elif SMOTE:\n",
    "        DATASET = 'smote'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/smote/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/smote/model_metrics_recall_ensemble.json'\n",
    "    elif SVMSMOTE:\n",
    "        DATASET = 'svmsmote'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/svmsmote/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/svmsmote/model_metrics_recall_ensemble.json'\n",
    "        if FEATURE_SELECT:\n",
    "            DATASET = 'FS_svmsmote'\n",
    "            FILENAME_ACC = 'EnsembleMetrics/svmsmote/model_metrics_accuracy_ensemble_FS.json'\n",
    "            FILENAME_RECALL = 'EnsembleMetrics/svmsmote/model_metrics_recall_ensemble_FS.json'\n",
    "            if SNV:\n",
    "                DATASET = 'snv_FS_svmsmote'\n",
    "                FILENAME_ACC = 'EnsembleMetrics/svmsmote/model_metrics_accuracy_ensemble_snv_FS.json'\n",
    "                FILENAME_RECALL = 'EnsembleMetrics/svmsmote/model_metrics_recall_ensemble_snv_FS.json'\n",
    "        if SNV:\n",
    "                DATASET = 'snv_svmsmote'\n",
    "                FILENAME_ACC = 'EnsembleMetrics/svmsmote/model_metrics_accuracy_ensemble_FS.json'\n",
    "                FILENAME_RECALL = 'EnsembleMetrics/svmsmote/model_metrics_recall_ensemble_FS.json'\n",
    "    elif KMEANSSMOTE:\n",
    "        DATASET = 'kmeanssmote'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/kmeanssmote/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/kmeanssmote/model_metrics_recall_ensemble.json'\n",
    "    elif ADASYNSMOTE:\n",
    "        DATASET = 'adasynsmote'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/adasynsmote/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/adasynsmote/model_metrics_recall_ensemble.json'\n",
    "    elif BORDERSMOTE:\n",
    "        DATASET = 'bordersmote'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/bordersmote/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/bordersmote/model_metrics_recall_ensemble.json'\n",
    "    elif MY_BALANCEDv2:\n",
    "        DATASET = 'my_balancedv2'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/my_balancedv2_dataset/model_metrics_accuracy_ensemble.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/my_balancedv2_dataset/model_metrics_recall_ensemble.json'\n",
    "    elif SNV:\n",
    "        DATASET = 'snv_raw'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/raw_dataset/model_metrics_accuracy_snv.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/raw_dataset/model_metrics_recall_snv.json'\n",
    "    elif FEATURE_SELECT:\n",
    "        DATASET = 'feature_select'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/selected_features/model_metrics_accuracy.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/selected_features/model_metrics_recall.json'\n",
    "    elif FEATURE_SELECTv2:\n",
    "        DATASET = 'feature_selectv2'\n",
    "        FILENAME_ACC = 'EnsembleMetrics/selected_features/model_metrics_accuracy2.json'\n",
    "        FILENAME_RECALL = 'EnsembleMetrics/selected_features/model_metrics_recall2.json'\n",
    "\n",
    "    # choose dataset and set x_train, x_test, y_train, y_test\n",
    "    if MY_BALANCED:\n",
    "        print('my_balanced')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/balanced_data/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/balanced_data/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/balanced_data/test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/balanced_data/test_label.csv', header = None)\n",
    "    elif BALANCED:\n",
    "        print('balanced')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/original_data/balanced_train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/original_data/balanced_train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/balanced_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/balanced_test_label.csv', header = None)\n",
    "    elif AUGMENTED:\n",
    "        print('augmented')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/augmented_data/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/augmented_data/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    elif AUGMENTEDv2:\n",
    "        print('augmentedv2')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/augmented_datav2/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/augmented_datav2/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    elif AUGMENTEDv3:\n",
    "        print('augmentedv3')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/augmented_datav3/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/augmented_datav3/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    elif MY_BALANCEDv2:\n",
    "        print('my_balancedv2')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/balancedv2_data/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/balancedv2_data/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    elif SMOTE:\n",
    "        print('smote')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/SMOTE/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/SMOTE/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    elif SVMSMOTE:\n",
    "        print('svmsmote')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/SMOTE/svm/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/SMOTE/svm/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    elif KMEANSSMOTE:\n",
    "        print('kmeanssmote')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/SMOTE/kmeans/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/SMOTE/kmeans/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    elif ADASYNSMOTE:\n",
    "        print('adasynsmote')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/SMOTE/adasyn/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/SMOTE/adasyn/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    elif BORDERSMOTE:\n",
    "        print('bordersmote')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/SMOTE/border/train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/SMOTE/border/train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "    else:\n",
    "        print('raw')\n",
    "        # x_train\n",
    "        training_data = pd.read_csv('../data/original_data/noExclusion_train_data.csv', header = None)\n",
    "        # y_train\n",
    "        training_labels = pd.read_csv('../data/original_data/noExclusion_train_label.csv', header = None)\n",
    "        # x_test\n",
    "        testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "        # y_test\n",
    "        testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "\n",
    "    # print(f\"training labels vc: \\n{training_labels.value_counts()}, \\ntesting labels vc: \\n{testing_labels.value_counts()}\")\n",
    "    # print(len(training_data), len(training_labels), len(testing_data), len(testing_labels))\n",
    "\n",
    "    # type cast labels to ints\n",
    "    training_labels[0] = training_labels[0].astype(int)\n",
    "    # testing_labels\n",
    "    testing_labels[0] = testing_labels[0].astype(int)\n",
    "\n",
    "    # encode labels, using sklearn, to pass to xgboost\n",
    "    # this code was inspired by the snippet from:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "    le = LabelEncoder()\n",
    "    # fit the classes to the encoder and transform labels to be 0,1,2\n",
    "    training_labels = le.fit_transform(training_labels[0].to_list())\n",
    "    testing_labels = le.fit_transform(testing_labels[0].to_list())\n",
    "\n",
    "    # bayessearch results\n",
    "    gs_results = {DATASET:{}}\n",
    "\n",
    "    # to store best ensemble for the experiment\n",
    "    best_ensembles[DATASET] = {'accuracy':-1}\n",
    "    # print(np.unique(training_labels))\n",
    "\n",
    "    # FEATURE SELECT\n",
    "    # RUN THIS TO APPLY FEATURE SELECTION TO TRAINING DATA\n",
    "    if FEATURE_SELECT == True:\n",
    "        ADD_POSSIBLE_FIGURES = True\n",
    "\n",
    "        # figures contain features from (figure_num*4)+1 \n",
    "        selected_figures = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\n",
    "                            21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,\n",
    "                            39,40,41,42,43,44,45,46,47,48,49,50,51,52]\n",
    "\n",
    "        # figures that MAY be decent - noisy but different peaks\n",
    "        possible_figures = [53,54,56,57,58,59,60,61,62,63,64,65]\n",
    "\n",
    "        # decent features - eyeballed\n",
    "\n",
    "        # generate set of selected features\n",
    "        selected_features = []\n",
    "\n",
    "        for figure_num in selected_figures:\n",
    "            for i in range(0,4):\n",
    "                # print(f\"figure: {figure_num}, feature: feature_{(figure_num*4)+i}\")\n",
    "                selected_features.append((figure_num*4)+i)\n",
    "\n",
    "        # to add possible features\n",
    "        if ADD_POSSIBLE_FIGURES == True:\n",
    "            for figure_num in possible_figures:\n",
    "                for i in range(0,4):\n",
    "                    # print(f\"figure: {figure_num}, feature: feature_{(figure_num*4)+i}\")\n",
    "                    selected_features.append((figure_num*4)+i)\n",
    "                    \n",
    "        training_data = training_data[selected_features]\n",
    "        testing_data = testing_data[selected_features]\n",
    "    else:\n",
    "        print('no feature selection')\n",
    "\n",
    "    # FSv2\n",
    "    if FEATURE_SELECTv2:    \n",
    "        # selected features round 2\n",
    "        selected_features = []\n",
    "        a = np.arange(30,85).tolist()\n",
    "        b = np.arange(203,235).tolist()\n",
    "\n",
    "        selected_features = np.concatenate([a,b]).tolist()\n",
    "        print(selected_features)\n",
    "        # (Cena, 2018)\n",
    "        training_data = training_data[selected_features]\n",
    "        testing_data = testing_data[selected_features]\n",
    "    else:\n",
    "        print('no FSv2')\n",
    "\n",
    "    # SNV\n",
    "    # apply SNV to training data - inspired by code from my ML CW\n",
    "    # (Hamzah Hafejee, 2022, COMP3611_Coursework_Assessment.ipynb, Comp 3611, University of Leeds)\n",
    "    # (Sklearn, 2023)\n",
    "    if SNV == True:\n",
    "        print(len(training_data))\n",
    "        # fit to training data\n",
    "        scaler = StandardScaler().fit(training_data)\n",
    "        training_data = scaler.transform(training_data)\n",
    "        testing_data = scaler.transform(testing_data)\n",
    "        print(\"After: \\n\", len(training_data))\n",
    "        len(training_data)\n",
    "    else:\n",
    "        print('no SNV standardisation')\n",
    "\n",
    "    #### train the models ####\n",
    "\n",
    "    # CART\n",
    "    # make CART classifier\n",
    "    clf_cart = tree.DecisionTreeClassifier(criterion=\"gini\", random_state=1)\n",
    "    # find optimal parameter values for CART\n",
    "\n",
    "    if DATASET == 'raw':\n",
    "        # raw dataset\n",
    "        params = {\n",
    "            'max_depth': [None], # control overfitting,\n",
    "            'max_features': ['log2'] # performance \n",
    "        }\n",
    "    elif DATASET == 'feature_select':\n",
    "        # selected + possible features\n",
    "        params = {\n",
    "            'max_depth': [5], # control overfitting,\n",
    "            'max_features': [None] # performance \n",
    "        }\n",
    "    elif DATASET == 'feature_selectv2':\n",
    "        # feature select v2\n",
    "        params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "    elif DATASET == 'snv_raw':\n",
    "        # SNV + raw\n",
    "        params = {'max_depth': [None], 'max_features': ['log2']} \n",
    "    elif DATASET == 'balanced':\n",
    "        params = {'max_depth': [10], 'max_features': ['sqrt']} \n",
    "    elif DATASET == 'snv_balanced':\n",
    "        params = {'max_depth': [10], 'max_features': ['sqrt']} \n",
    "\n",
    "    elif DATASET == 'my_balanced':\n",
    "        params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "\n",
    "    elif DATASET == 'FS_my_balanced':\n",
    "        params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "        \n",
    "    elif DATASET == 'FSv2_my_balanced':\n",
    "        params = {'max_depth': [10], 'max_features': ['log2']} \n",
    "        \n",
    "    elif DATASET == 'snv_my_balanced':\n",
    "        params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "\n",
    "    elif DATASET == 'snv_FS_my_balanced':\n",
    "        params = {'max_depth': [5], 'max_features': [None]} \n",
    "    \n",
    "    elif DATASET == 'augmented':\n",
    "        params = {'max_depth': [10], 'max_features': ['log2']} \n",
    "\n",
    "    elif DATASET == 'augmented_FS':\n",
    "        params = {'max_depth': [10], 'max_features': ['sqrt']} \n",
    "\n",
    "    elif DATASET == 'smote':\n",
    "        params = {'max_depth': [15], 'max_features': ['sqrt']} \n",
    "\n",
    "    elif DATASET == 'augmentedv3':\n",
    "        params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "        \n",
    "    elif DATASET == 'snv_augmentedv3':\n",
    "        params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "    \n",
    "    elif DATASET == 'snv_FS_balanced':\n",
    "        params = {'max_depth': [10], 'max_features': [None]} \n",
    "\n",
    "    elif DATASET == 'augmentedv3_FS':\n",
    "        params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "\n",
    "    elif DATASET == 'kmeanssmote':\n",
    "        params = {'max_depth': [10], 'max_features': ['sqrt']} \n",
    "        \n",
    "    elif DATASET == 'svmsmote':\n",
    "        params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "        \n",
    "    elif DATASET == 'adasynsmote':\n",
    "        params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "        \n",
    "    elif DATASET == 'bordersmote':\n",
    "        params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "        \n",
    "    elif DATASET == 'snv_svmsmote':\n",
    "        params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "        \n",
    "    elif DATASET == 'snv_FS_svmsmote':\n",
    "        params = {'max_depth': [10], 'max_features': [None]} \n",
    "        \n",
    "    grid_search = GridSearchCV(clf_cart, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, np.ravel(training_labels))\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_cart = grid_search.best_estimator_\n",
    "    gs_results['CART'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "    # make Gaussian Naive Bayes classifier\n",
    "    clf_nb = GaussianNB()\n",
    "    params = {\n",
    "        'var_smoothing':[1e-20, 1e-19, 1e-18, 1e-17, 1e-16, 1e-15, 1e-14, 1e-13, 1e-12, 1e-11, 1e-10, 1e-9], # from less smoothing to more aggressive smoothing\n",
    "    }\n",
    "    grid_search = GridSearchCV(clf_nb, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, np.ravel(training_labels))\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_nb = grid_search.best_estimator_\n",
    "    gs_results['GNB'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "    # make k-Nearest Neighbours classifier\n",
    "    clf_knn = KNeighborsClassifier(n_jobs=-1) # use all processes for parellelisation\n",
    "    params = {\n",
    "        'n_neighbors': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    }\n",
    "    grid_search = GridSearchCV(clf_knn, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, np.ravel(training_labels))\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_knn = grid_search.best_estimator_\n",
    "    gs_results['kNN'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "    # make SVM-RBF classifier\n",
    "    clf_svmrbf = SVC(kernel='rbf', random_state=1)\n",
    "\n",
    "    if DATASET == 'raw':\n",
    "        # raw dataset\n",
    "        params = {\n",
    "            'C': [100], # high to low regularisation strength\n",
    "            'gamma' : ['scale'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_select':\n",
    "        # selected + possible features\n",
    "        params = {\n",
    "            'C': [1000], # high to low regularisation strength\n",
    "            'gamma' : ['scale'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_selectv2':\n",
    "        # selected + possible features v2\n",
    "        params = {\n",
    "            'C': [1000], # high to low regularisation strength\n",
    "            'gamma' : ['scale'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'snv_raw':\n",
    "        # SNV + raw\n",
    "        params = {'C': [10], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'balanced':\n",
    "        params = {'C': [1000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_balanced':\n",
    "        params = {'C': [1000], 'gamma': ['scale']}\n",
    "\n",
    "    elif DATASET == 'my_balanced':\n",
    "        params = {'C': [100], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'FS_my_balanced':\n",
    "        params = {'C': [100], 'gamma':[ 'scale']} \n",
    "\n",
    "    elif DATASET == 'FSv2_my_balanced':\n",
    "        params = {'C': [5000], 'gamma':[ 'scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_my_balanced':\n",
    "        params = {'C': [100], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'snv_FS_my_balanced':\n",
    "        params = {'C': [5000], 'gamma':[ 'scale']} \n",
    "        \n",
    "    elif DATASET == 'augmented':\n",
    "        params = {'C': [5000], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'augmented_FS':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'smote':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'augmentedv3':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_augmentedv3':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_balanced':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'augmentedv3_FS':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'kmeanssmote':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'svmsmote':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'adasynsmote':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'bordersmote':\n",
    "        params = {'C': [5000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_svmsmote':\n",
    "        params = {'C': [1000], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_svmsmote':\n",
    "        params = {'C': [5000], 'gamma': ['auto']} \n",
    "        \n",
    "    grid_search = GridSearchCV(clf_svmrbf, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, np.ravel(training_labels))\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_svmrbf = grid_search.best_estimator_\n",
    "    gs_results['SVM-RBF'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "    # make SVM linear classifier\n",
    "    clf_lin = SVC(kernel='linear', random_state=1)\n",
    "    params = {\n",
    "        'C': [0.05, 0.1, 1, 10, 100, 1000], # high to low regularisation strength\n",
    "        'gamma' : ['scale', 'auto'], # need to research this parameter more\n",
    "        # 'gamma' : [], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "    if DATASET == 'raw':\n",
    "        # raw dataset\n",
    "        params = {\n",
    "            'C': [10], # high to low regularisation strength\n",
    "            'gamma' : ['scale'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_select':\n",
    "        # selected + possible features\n",
    "        params = {\n",
    "            'C': [1], # high to low regularisation strength\n",
    "            'gamma' : ['scale'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_selectv2':\n",
    "        # FSv2\n",
    "        params = {\n",
    "            'C': [1], # high to low regularisation strength\n",
    "            'gamma' : ['scale'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'snv_raw':\n",
    "        # SNV + raw\n",
    "        params = {'C': [0.1], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'balanced':\n",
    "        params = {'C': [1], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'snv_balanced':\n",
    "        params = {'C': [0.1], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'my_balanced':\n",
    "        params = {'C': [10], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'FS_my_balanced':\n",
    "        params = {'C': [10], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'FSv2_my_balanced':\n",
    "        params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_my_balanced':\n",
    "        params = {'C': [0.05], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'snv_FS_my_balanced':\n",
    "        params = {'C': [0.1], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'augmented':\n",
    "        params = {'C': [10], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'augmented_FS':\n",
    "        params = {'C': [100], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'smote':\n",
    "        params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "        \n",
    "    elif DATASET == 'augmentedv3':\n",
    "        params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "\n",
    "    elif DATASET == 'snv_augmentedv3':\n",
    "        params = {'C': [100], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'snv_FS_balanced':\n",
    "        params = {'C': [1], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'augmentedv3_FS':\n",
    "        params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "        \n",
    "    elif DATASET == 'svmsmote':\n",
    "        params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "            \n",
    "    elif DATASET == 'kmeanssmote':\n",
    "        params = {'C': [10], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'adasynsmote':\n",
    "        params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "        \n",
    "    elif DATASET == 'bordersmote':\n",
    "        params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_svmsmote':\n",
    "        params = {'C': [100], 'gamma':[ 'scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_svmsmote':\n",
    "        params = {'C': [100], 'gamma':[ 'scale']} \n",
    "        \n",
    "    grid_search = GridSearchCV(clf_lin, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, np.ravel(training_labels))\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_svmlin = grid_search.best_estimator_\n",
    "    gs_results['SVM-Lin'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "    # make svm sigmoidal classifier\n",
    "    clf_sig = SVC(kernel='sigmoid', random_state=1)\n",
    "\n",
    "    if DATASET == 'raw':\n",
    "        # raw dataset\n",
    "        params = {\n",
    "            'C': [10], # high to low regularisation strength\n",
    "            'gamma' : ['auto'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_select':\n",
    "        # selected + possible features\n",
    "        params = {\n",
    "            'C': [1e-05], # high to low regularisation strength\n",
    "            'gamma' : ['scale'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_selectv2':\n",
    "        # FSv2\n",
    "        params = {\n",
    "            'C': [0.1], # high to low regularisation strength\n",
    "            'gamma' : ['auto'], # need to research this parameter more\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'snv_raw':\n",
    "        # SNV + raw\n",
    "        params = {'C': [0.1], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'balanced':\n",
    "        params = {'C': [0.1], 'gamma': ['auto']}\n",
    "\n",
    "    elif DATASET == 'snv_balanced':\n",
    "        params = {'C': [0.1], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'my_balanced':\n",
    "        params = {'C': [10], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'FS_my_balanced':\n",
    "        params = {'C': [10], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'FSv2_my_balanced':\n",
    "        params = {'C': [1], 'gamma':[ 'auto']} \n",
    "        \n",
    "    elif DATASET == 'snv_my_balanced':\n",
    "        params = {'C': [0.1], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'snv_FS_my_balanced':\n",
    "        params = {'C': [0.1], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'augmented':\n",
    "        params = {'C': [100], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'augmented_FS':\n",
    "        params = {'C': [100], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'smote':\n",
    "        params = {'C': [0.1], 'gamma': ['auto']} \n",
    "        \n",
    "    elif DATASET == 'augmentedv3':\n",
    "        params = {'C': [100], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'snv_augmentedv3':\n",
    "        params = {'C': [0.01], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'snv_FS_balanced':\n",
    "        params = {'C': [1], 'gamma': ['scale']} \n",
    "\n",
    "    elif DATASET == 'augmentedv3_FS':\n",
    "        params = {'C': [0.1], 'gamma': ['auto']} \n",
    "\n",
    "    elif DATASET == 'svmsmote':\n",
    "        params = {'C': [0.1], 'gamma': ['auto']} \n",
    "        \n",
    "    elif DATASET == 'kmeanssmote':\n",
    "        params = {'C': [100], 'gamma': ['auto']} \n",
    "        \n",
    "    elif DATASET == 'adasynsmote':\n",
    "        params = {'C': [0.1], 'gamma': ['auto']} \n",
    "        \n",
    "    elif DATASET == 'bordersmote':\n",
    "        params = {'C': [0.1], 'gamma': ['auto']} \n",
    "        \n",
    "    elif DATASET == 'snv_svmsmote':\n",
    "        params = {'C': [0.1], 'gamma': ['scale']} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_svmsmote':\n",
    "        params = {'C': [0.1], 'gamma': ['scale']} \n",
    "        \n",
    "    grid_search = GridSearchCV(clf_sig, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, np.ravel(training_labels))\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_svmsig = grid_search.best_estimator_\n",
    "    gs_results['SVM-Sig'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "    # make xgboost classifier (Piotr Płoński, 2021) (Jain, 2016)\n",
    "    clf_xgb = xgb.XGBClassifier(random_state = 1)\n",
    "\n",
    "\n",
    "    if DATASET == 'raw':\n",
    "        # raw dataset\n",
    "        params = {\n",
    "            'n_estimators': [100], # no. boosting rounds\n",
    "            'max_depth': [10] # control overfitting\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_select':\n",
    "        # selected + possible features\n",
    "        params = {\n",
    "            'n_estimators': [10], # no. boosting rounds\n",
    "            'max_depth': [5] # control overfitting\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_selectv2':\n",
    "        # FSv2\n",
    "        params = {\n",
    "            'n_estimators': [100], # no. boosting rounds\n",
    "            'max_depth': [5] # control overfitting\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'snv_raw':\n",
    "        # SNV + raw\n",
    "        params = {'max_depth': [10], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'balanced':\n",
    "        params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'snv_balanced':\n",
    "        params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "        \n",
    "    elif DATASET == 'my_balanced':\n",
    "        params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'FS_my_balanced':\n",
    "        params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'FSv2_my_balanced':\n",
    "        params = {'max_depth': [15], 'n_estimators': [500]} \n",
    "        \n",
    "    elif DATASET == 'snv_my_balanced':\n",
    "        params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'snv_FS_my_balanced':\n",
    "        params = {'max_depth': [10], 'n_estimators': [100]} \n",
    "        \n",
    "    elif DATASET == 'augmented':\n",
    "        params = {'max_depth': [5], 'n_estimators': [500]} \n",
    "\n",
    "    elif DATASET == 'augmented_FS':\n",
    "        params = {'max_depth': [10], 'n_estimators': [1000]} \n",
    "\n",
    "    elif DATASET == 'smote':\n",
    "        params = {'max_depth': [7], 'n_estimators': [500]} \n",
    "\n",
    "    elif DATASET == 'augmentedv3':\n",
    "        params = {'max_depth': [3], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'snv_augmentedv3':\n",
    "        params = {'max_depth': [3], 'n_estimators': [100]} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_balanced':\n",
    "        params = {'max_depth': [5], 'n_estimators': [500]} \n",
    "\n",
    "    elif DATASET == 'augmentedv3_FS':\n",
    "        params = {'max_depth': [7], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'svmsmote':\n",
    "        params = {'max_depth': [5], 'n_estimators': [500]} \n",
    "        \n",
    "    elif DATASET == 'kmeanssmote':\n",
    "        params = {'max_depth': [10], 'n_estimators': [500]} \n",
    "        \n",
    "    elif DATASET == 'adasynsmote':\n",
    "        params = {'max_depth': [5], 'n_estimators': [1000]} \n",
    "        \n",
    "    elif DATASET == 'bordersmote':\n",
    "        params = {'max_depth': [5], 'n_estimators': [100]} \n",
    "        \n",
    "    elif DATASET == 'snv_svmsmote':\n",
    "        params = {'max_depth': [5], 'n_estimators': [500]} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_svmsmote':\n",
    "        params = {'max_depth': [3], 'n_estimators': [1000]} \n",
    "        \n",
    "    grid_search = GridSearchCV(clf_xgb, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, training_labels)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_xgb = grid_search.best_estimator_\n",
    "    gs_results['XGB'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "    # make adaboost classifier\n",
    "    clf_ada = AdaBoostClassifier(random_state=1)\n",
    "\n",
    "    if DATASET == 'raw':\n",
    "        # raw dataset\n",
    "        params = {\n",
    "            'n_estimators': [50],\n",
    "            'learning_rate': [0.01] # weight applied to each clf at each boosting iteration\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_select':\n",
    "        # selected + possible features\n",
    "        params = {\n",
    "            'n_estimators': [10],\n",
    "            'learning_rate': [0.01] # weight applied to each clf at each boosting iteration\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_selectv2':\n",
    "        # FSv2\n",
    "        params = {\n",
    "            'n_estimators': [50],\n",
    "            'learning_rate': [0.01] # weight applied to each clf at each boosting iteration\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'snv_raw':\n",
    "        # SNV + raw\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [50]} \n",
    "\n",
    "    elif DATASET == 'balanced':\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [10]} \n",
    "\n",
    "    elif DATASET == 'snv_balanced':\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [10]} \n",
    "        \n",
    "    elif DATASET == 'my_balanced':\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'FS_my_balanced':\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'FSv2_my_balanced':\n",
    "        params = {'learning_rate': [0.1], 'n_estimators': [10]} \n",
    "        \n",
    "    elif DATASET == 'snv_my_balanced':\n",
    "        params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "\n",
    "    elif DATASET == 'snv_FS_my_balanced':\n",
    "        params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "        \n",
    "    elif DATASET == 'augmented':\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [50]} \n",
    "\n",
    "    elif DATASET == 'augmented_FS':\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [50]} \n",
    "\n",
    "    elif DATASET == 'smote':\n",
    "        params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "        \n",
    "    elif DATASET == 'augmentedv3':\n",
    "        params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "\n",
    "    elif DATASET == 'snv_augmentedv3':\n",
    "        params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "            \n",
    "    elif DATASET == 'snv_FS_balanced':\n",
    "        params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "\n",
    "    elif DATASET == 'augmentedv3_FS':\n",
    "        params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "        \n",
    "    elif DATASET == 'svmsmote':\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [1000]} \n",
    "        \n",
    "    elif DATASET == 'kmeanssmote':\n",
    "        params = {'learning_rate': [0.1], 'n_estimators': [10]} \n",
    "        \n",
    "    elif DATASET == 'adasynsmote':\n",
    "        params = {'learning_rate': [1], 'n_estimators': [500]} \n",
    "        \n",
    "    elif DATASET == 'bordersmote':\n",
    "        params = {'learning_rate': [1], 'n_estimators': [500]} \n",
    "        \n",
    "    elif DATASET == 'snv_svmsmote':\n",
    "        params = {'learning_rate': [0.01], 'n_estimators': [1000]} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_svmsmote':\n",
    "        params = {'learning_rate': [0.1], 'n_estimators': [10]} \n",
    "        \n",
    "    grid_search = GridSearchCV(clf_ada, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, training_labels)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_ada = grid_search.best_estimator_\n",
    "    gs_results['ADA'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "    # # make Logistic Regressor\n",
    "    # make Logistic Regressor\n",
    "    clf_lr = LogisticRegression(random_state=1, max_iter=1000)\n",
    "\n",
    "    if DATASET == 'raw':\n",
    "        # raw dataset\n",
    "        params = {\n",
    "            'penalty': ['l2'], # type of regularisation \n",
    "            'C': [0.1], # regularisation strength\n",
    "            'solver': ['lbfgs'] # approach to finding best weights\n",
    "        }\n",
    "    elif DATASET == 'feature_select':\n",
    "        # selected + possible features\n",
    "        params = {\n",
    "            'penalty': ['l2'], # type of regularisation \n",
    "            'C': [0.1], # regularisation strength\n",
    "            'solver': ['newton-cg'] # approach to finding best weights\n",
    "        }\n",
    "    elif DATASET == 'feature_selectv2':\n",
    "        # FSv2\n",
    "        params = {'C': [1], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "    elif DATASET == 'snv_raw':\n",
    "        # SNV + raw\n",
    "        params = {'C': [0.1], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "    elif DATASET == 'balanced':\n",
    "        params = {'C': [100], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "    elif DATASET == 'snv_balanced':\n",
    "        params = {'C': [0.1], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "\n",
    "    elif DATASET == 'my_balanced':\n",
    "        params = {'C': [10], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "        \n",
    "    elif DATASET == 'FS_my_balanced':\n",
    "        params = {'C': [10], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "        \n",
    "    elif DATASET == 'FSv2_my_balanced':\n",
    "        params = {'C': [10], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "        \n",
    "    elif DATASET == 'snv_my_balanced':\n",
    "        params = {'C': [1], 'penalty': ['l1'], 'solver': ['saga']} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_my_balanced':\n",
    "        params = {'C': [1], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "        \n",
    "    elif DATASET == 'augmented':\n",
    "        params = {'C': [10], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "        \n",
    "    elif DATASET == 'augmented_FS':\n",
    "        params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "\n",
    "    elif DATASET == 'smote':\n",
    "        params = {'C': [10], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "\n",
    "    elif DATASET == 'augmentedv3':\n",
    "        params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "        \n",
    "    elif DATASET == 'snv_augmentedv3':\n",
    "        params = {'C': [10], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_balanced':\n",
    "        params = {'C': [10], 'penalty': ['l1'], 'solver': ['saga']} \n",
    "\n",
    "    elif DATASET == 'augmentedv3_FS':\n",
    "        params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "        \n",
    "    elif DATASET == 'svmsmote':\n",
    "        params = {'C': [100], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "        \n",
    "    elif DATASET == 'kmeanssmote':\n",
    "        params = {'C': [10], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "        \n",
    "    elif DATASET == 'adasynsmote':\n",
    "        params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "        \n",
    "    elif DATASET == 'bordersmote':\n",
    "        params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "        \n",
    "    elif DATASET == 'snv_svmsmote':\n",
    "        params = {'C': [100], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_svmsmote':\n",
    "        params = {'C': [100], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "        \n",
    "    grid_search = GridSearchCV(clf_lr, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, np.ravel(training_labels))\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    gs_results['LR'] = {'accuracy':best_score, 'params':best_params}\n",
    "    best_lr = grid_search.best_estimator_\n",
    "\n",
    "    \n",
    "    # make Random Forest classifier\n",
    "    # make Random Forest classifier\n",
    "    clf_rf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "    if DATASET == 'raw':\n",
    "        # raw dataset\n",
    "        params = {\n",
    "        'n_estimators': [50],\n",
    "        'max_depth': [10],\n",
    "        \"max_features\" : [30]\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_select':\n",
    "        # selected + possible features\n",
    "        params = {\n",
    "        'n_estimators': [200],\n",
    "        'max_depth': [None],\n",
    "        \"max_features\" : [5]\n",
    "        }\n",
    "\n",
    "    elif DATASET == 'feature_selectv2':\n",
    "        # feature select v2\n",
    "        params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [300]}\n",
    "\n",
    "    elif DATASET == 'snv_raw':\n",
    "        # SNV + raw\n",
    "        params = {'max_depth': [10], 'max_features': [30], 'n_estimators': [50]} \n",
    "\n",
    "    elif DATASET == 'balanced':\n",
    "        params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'snv_balanced':\n",
    "        params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [100]} \n",
    "        \n",
    "    elif DATASET == 'my_balanced':\n",
    "        params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [300]} \n",
    "\n",
    "    elif DATASET == 'FS_my_balanced':\n",
    "        params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [300]} \n",
    "\n",
    "    elif DATASET == 'FSv2_my_balanced':\n",
    "        params = {'max_depth': [None], 'max_features': [30], 'n_estimators': [100]} \n",
    "        \n",
    "    elif DATASET == 'snv_my_balanced':\n",
    "        params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [300]}\n",
    "\n",
    "    elif DATASET == 'snv_FS_my_balanced':\n",
    "        params = {'max_depth': [None], 'max_features': [10], 'n_estimators': [10]} \n",
    "\n",
    "    elif DATASET == 'augmented':\n",
    "        params = {'max_depth': [None], 'max_features': [20], 'n_estimators': [100]} \n",
    "\n",
    "    elif DATASET == 'augmented_FS':\n",
    "        params = {'max_depth': [None], 'max_features': [1], 'n_estimators': [50]} \n",
    "\n",
    "    elif DATASET == 'smote':\n",
    "        params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [50]} \n",
    "\n",
    "    elif DATASET == 'augmentedv3':\n",
    "        params = {'max_depth': [None], 'max_features': [10], 'n_estimators': [200]} \n",
    "\n",
    "    elif DATASET == 'snv_augmentedv3':\n",
    "        params = {'max_depth': [None], 'max_features': [10], 'n_estimators': [200]} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_balanced':\n",
    "        params = {'max_depth': [None], 'max_features': [10], 'n_estimators': [300]} \n",
    "        \n",
    "    elif DATASET == 'augmentedv3_FS':\n",
    "        params = {'max_depth': [10], 'max_features': [1], 'n_estimators': [300]} \n",
    "\n",
    "    elif DATASET == 'svmsmote':\n",
    "        params = {'max_depth': [None], 'max_features': [20], 'n_estimators': [50]} \n",
    "        \n",
    "    elif DATASET == 'kmeanssmote':\n",
    "        params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [100]} \n",
    "        \n",
    "    elif DATASET == 'adasynsmote':\n",
    "        params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [50]} \n",
    "        \n",
    "    elif DATASET == 'bordersmote':\n",
    "        params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [50]} \n",
    "        \n",
    "    elif DATASET == 'snv_svmsmote':\n",
    "        params = {'max_depth': [None], 'max_features': [20], 'n_estimators': [50]} \n",
    "        \n",
    "    elif DATASET == 'snv_FS_svmsmote':\n",
    "        params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [100]} \n",
    "        \n",
    "    grid_search  = GridSearchCV(clf_rf, params, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(training_data, np.ravel(training_labels))\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    gs_results['RF'] = {'accuracy':best_score, 'params':best_params}\n",
    "\n",
    "\n",
    "    # ensemble model (Sklearn, 2014), (Sklearn, 2023)\n",
    "    ### FIND BEST ENSEMBLE MODEL FOR THE EXPERIMENT ###\n",
    "    models_dict = {'CART':best_cart,\n",
    "                'RF':best_rf,\n",
    "                'LR':best_lr, \n",
    "                'GNB': best_nb,\n",
    "                'kNN': best_knn, \n",
    "                'SVM-RBF': best_svmrbf, \n",
    "                'SVM-Lin': best_svmlin, \n",
    "                'SVM-Sig': best_svmsig, \n",
    "                'XGB': best_xgb, \n",
    "                'ADA': best_ada, \n",
    "                }\n",
    "    best_ensemble = None\n",
    "\n",
    "    # from every model, choose 5 (nCr)\n",
    "    combinations = list(itertools.combinations(models_dict,5))\n",
    "    # for each 5-way combination of estimators\n",
    "    for combi in combinations:\n",
    "        estimators = []\n",
    "        # print(combi)\n",
    "        for model in combi:\n",
    "            # print(model)\n",
    "            estimators.append((model, models_dict[model]))\n",
    "        # create ensemble\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=estimators,\n",
    "            voting='hard',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        ensemble.fit(training_data, training_labels)\n",
    "        accuracy = ensemble.score(testing_data, testing_labels)\n",
    "        # accuracies.append(accuracy)\n",
    "        if accuracy > best_ensembles[DATASET]['accuracy']:\n",
    "            best_ensembles[DATASET]['accuracy'] = accuracy\n",
    "            best_ensembles[DATASET]['estimators'] = str(ensemble.estimators_)\n",
    "            best_ensemble = ensemble\n",
    "        # print(combi, accuracy)\n",
    "\n",
    "\n",
    "\n",
    "    # evaluate models (Sklearn, 2023)\n",
    "    # model_metrics = {'accuracy', 'recall', 'precision', 'F1-score', 'ROC-AUC'}\n",
    "    model_metrics = {}\n",
    "\n",
    "    # all of the models\n",
    "    models = [best_cart, best_rf, best_nb, best_knn, best_svmrbf, best_svmlin, best_svmsig, best_xgb, best_ada, best_ensemble]\n",
    "    model_names = ['CART', 'RF', 'GNB', 'kNN', 'SVM-RBF', 'SVM-Lin', 'SVM-Sig', 'XGB', 'ADA', 'Ensemble']\n",
    "    i=0\n",
    "    for model in models:\n",
    "        # train on test set\n",
    "        predicted = model.predict(testing_data)\n",
    "        # generate cm against test labels\n",
    "        cm = confusion_matrix(testing_labels, predicted)\n",
    "        # print(cm)\n",
    "        accuracy = accuracy_score(testing_labels, predicted)\n",
    "        recall = recall_score(testing_labels, predicted, average=None)\n",
    "        precision = precision_score(testing_labels, predicted, average=None)\n",
    "        f1 = f1_score(testing_labels, predicted, average=None)\n",
    "\n",
    "        try:\n",
    "            predicted_prob = model.predict_proba(testing_data)\n",
    "            roc = roc_auc_score(testing_labels, predicted_prob, average=None, multi_class='ovr') \n",
    "            # print(accuracy, recall, precision, f1, roc)\n",
    "            model_metrics[model_names[i]] = {\n",
    "                                                'accuracy':accuracy, \n",
    "                                                'recall':{\n",
    "                                                    1:recall[0], \n",
    "                                                    2:recall[1], \n",
    "                                                    3:recall[2]\n",
    "                                                },\n",
    "                                                'precision':{\n",
    "                                                    1:precision[0], \n",
    "                                                    2:precision[1], \n",
    "                                                    3:precision[2]\n",
    "                                                },\n",
    "                                                'f1_score':{\n",
    "                                                    1:f1[0], \n",
    "                                                    2:f1[1], \n",
    "                                                    3:f1[2]\n",
    "                                                },\n",
    "                                                'ROC-AUC':{\n",
    "                                                    1:roc[0], \n",
    "                                                    2:roc[1], \n",
    "                                                    3:roc[2]\n",
    "                                                }\n",
    "            }\n",
    "        except:\n",
    "            # print(f\"can't predict class probilities for {model_names[i]}\")\n",
    "            # print(accuracy, recall, precision, f1)\n",
    "            model_metrics[model_names[i]] = {\n",
    "                                                'accuracy':accuracy, \n",
    "                                                'recall':{\n",
    "                                                    1:recall[0], \n",
    "                                                    2:recall[1], \n",
    "                                                    3:recall[2]\n",
    "                                                },\n",
    "                                                'precision':{\n",
    "                                                    1:precision[0], \n",
    "                                                    2:precision[1], \n",
    "                                                    3:precision[2]\n",
    "                                                },\n",
    "                                                'f1_score':{\n",
    "                                                    1:f1[0], \n",
    "                                                    2:f1[1], \n",
    "                                                    3:f1[2]\n",
    "                                                }\n",
    "            }\n",
    "        i+=1\n",
    "\n",
    "    # (Gern Blanston, 2009)- sort by neoplasia recall\n",
    "    sorted_metrics = dict(sorted(model_metrics.items(), key=lambda item: item[1]['recall'][3], reverse=True))\n",
    "    # (holys, 2013)\n",
    "    with open(FILENAME_RECALL, 'w+') as fp:\n",
    "        json.dump(sorted_metrics, fp)\n",
    "        fp.close()\n",
    "\n",
    "    # redo but sort by accuracy\n",
    "    # (Gern Blanston, 2009)\n",
    "    sorted_metrics_acc = dict(sorted(model_metrics.items(), key=lambda item: item[1]['accuracy'], reverse=True))\n",
    "    # (holys, 2013)\n",
    "    with open(FILENAME_ACC, 'w+') as fp:\n",
    "        json.dump(sorted_metrics_acc, fp)\n",
    "        fp.close()\n",
    "\n",
    "\n",
    "\n",
    "    # (Neekhara, 2019)\n",
    "    # DATASET = 'snv_svmsmote'\n",
    "    # function to add to JSON\n",
    "    def write_json(new_data, ds, filename='EnsembleMetrics/scoreboard.json'):\n",
    "        with open(filename,'r+') as file:\n",
    "            # First we load existing data into a dict.\n",
    "            file_data = json.load(file)\n",
    "            # Join new_data with file_data inside emp_details\n",
    "            file_data[ds] = (new_data)\n",
    "            file_data = dict(sorted(file_data.items(), key=lambda item: item[1]['all']['accuracy'], reverse=True))\n",
    "            # Sets file's current position at offset.\n",
    "            file.seek(0)\n",
    "            # convert back to json.\n",
    "            json.dump(file_data, file, indent = 4)\n",
    "            file.close()\n",
    "\n",
    "    # calculate avg accuracy and recall\n",
    "    accuracy = 0\n",
    "    recall = 0\n",
    "    count = 0\n",
    "    for model in model_names:\n",
    "        # print(model_metrics[model]['accuracy'])\n",
    "        accuracy += model_metrics[model]['accuracy']\n",
    "        recall += model_metrics[model]['recall'][3]\n",
    "        count +=1\n",
    "\n",
    "    t6acc = 0\n",
    "    t6rec = 0\n",
    "    count2 = 0\n",
    "    for key in sorted_metrics_acc:\n",
    "        if count2 ==6:\n",
    "            break\n",
    "        t6acc += sorted_metrics_acc[key]['accuracy']\n",
    "        t6rec += sorted_metrics_acc[key]['recall'][3]\n",
    "        count2 +=1\n",
    "\n",
    "    avg = {'all' : {'accuracy': accuracy/count, 'neoplasia recall': recall/count},\n",
    "        'top6' : {'accuracy': t6acc/count2, 'recall':t6rec/count2}}\n",
    "    # print(accuracy, recall, count)\n",
    "    # print(t6acc, t6rec, count2)\n",
    "    if DATASET != 'test':\n",
    "        write_json(avg, ds=DATASET)\n",
    "    else:\n",
    "        print(DATASET)\n",
    "\n",
    "    with open('best_ensembles.json', 'w+') as fp:\n",
    "        json.dump(best_ensembles, fp, indent=4)\n",
    "        fp.close()\n",
    "\n",
    "    # print(f\"{DATASET} max accuracy: {max(accuracies)}\")\n",
    "    \n",
    "# after all experiments done, put the list of best ensembles into a json file (Moobi, 2013)\n",
    "with open('best_ensembles.json', 'w+') as fp:\n",
    "    json.dump(best_ensembles, fp, indent=4)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_ensembles.json', 'w+') as fp:\n",
    "    json.dump(best_ensembles, fp, indent=4)\n",
    "    fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'snv_svmsmote': {'accuracy': 0.8531468531468531,\n",
       "  'estimators': \"[DecisionTreeClassifier(max_features='log2', random_state=1), RandomForestClassifier(max_features=20, n_estimators=50, random_state=1), LogisticRegression(C=100, max_iter=1000, random_state=1), KNeighborsClassifier(n_jobs=-1, n_neighbors=1), SVC(C=1000, random_state=1)]\"}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ensemble.estimators_)\n",
    "best_ensembles[DATASET]['estimators'] = str(best_ensembles[DATASET]['estimators'])\n",
    "best_ensembles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69968d942e1dcc7d0770d34dbcb974701730c09224194d51fbd302d9296a213d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
