{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import os, json\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score, confusion_matrix, accuracy_score, f1_score, precision_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train\n",
    "training_data = pd.read_csv('../data/original_data/noExclusion_train_data.csv', header = None)\n",
    "# y_train\n",
    "training_labels = pd.read_csv('../data/original_data/noExclusion_train_label.csv', header = None)\n",
    "# x_test\n",
    "testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "# y_test\n",
    "testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type cast labels to ints\n",
    "training_labels[0] = training_labels[0].astype(int)\n",
    "# testing_labels\n",
    "testing_labels[0] = testing_labels[0].astype(int)\n",
    "\n",
    "# encode labels, using sklearn, to pass to xgboost\n",
    "# this code was inspired by the snippet from:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "le = LabelEncoder()\n",
    "# fit the classes to the encoder and transform labels\n",
    "training_labels = le.fit_transform(training_labels[0].to_list())\n",
    "testing_labels = le.fit_transform(testing_labels[0].to_list())\n",
    "\n",
    "# gridsearch results\n",
    "gs_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS TO APPLY FEATURE SELECTION TO TRAINING DATA\n",
    "ADD_POSSIBLE_FIGURES = True\n",
    "\n",
    "# figures contain features from (figure_num*4)+1 \n",
    "selected_figures = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\n",
    "                    21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,\n",
    "                    39,40,41,42,43,44,45,46,47,48,49,50,51,52]\n",
    "\n",
    "# figures that MAY be decent - noisy but different peaks\n",
    "possible_figures = [53,54,56,57,58,59,60,61,62,63,64,65]\n",
    "\n",
    "# decent features - eyeballed\n",
    "\n",
    "# generate set of selected features\n",
    "selected_features = []\n",
    "\n",
    "for figure_num in selected_figures:\n",
    "    for i in range(0,4):\n",
    "        # print(f\"figure: {figure_num}, feature: feature_{(figure_num*4)+i}\")\n",
    "        selected_features.append((figure_num*4)+i)\n",
    "\n",
    "# to add possible features\n",
    "if ADD_POSSIBLE_FIGURES == True:\n",
    "    for figure_num in possible_figures:\n",
    "        for i in range(0,4):\n",
    "            # print(f\"figure: {figure_num}, feature: feature_{(figure_num*4)+i}\")\n",
    "            selected_features.append((figure_num*4)+i)\n",
    "\n",
    "# (Cena, 2018)\n",
    "training_data = training_data[selected_features]\n",
    "testing_data = testing_data[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'max_depth': 5, 'max_features': None} \n",
      "best_score: 0.8076527525710828\n"
     ]
    }
   ],
   "source": [
    "# make CART classifier\n",
    "clf_cart = tree.DecisionTreeClassifier(criterion=\"gini\", random_state=1)\n",
    "# find optimal parameter values for CART\n",
    "params = {\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40], # control overfitting,\n",
    "    'max_features': [None, 'sqrt', 'log2'] # performance \n",
    "}\n",
    "\n",
    "params = {\n",
    "    'max_depth': [5], # control overfitting,\n",
    "    'max_features': [None] # performance \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_cart, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_cart = grid_search.best_estimator_\n",
    "gs_results['CART'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'max_depth': None, 'max_features': 5, 'n_estimators': 200} \n",
      "best_score: 0.8514216575922564\n"
     ]
    }
   ],
   "source": [
    "# make Random Forest classifier\n",
    "clf_rf = RandomForestClassifier(random_state=1)\n",
    "params = {\n",
    "  'n_estimators': [10, 50, 100, 200, 300],\n",
    "  'max_depth': [None, 5, 10, 20, 30, 40],\n",
    "  \"max_features\" : [None, 1, 5, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# so it doesn't redo lengthy GS\n",
    "params = {\n",
    "  'n_estimators': [200],\n",
    "  'max_depth': [None],\n",
    "  \"max_features\" : [5]\n",
    "}\n",
    "\n",
    "grid_search  = GridSearchCV(clf_rf, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_rf = grid_search.best_estimator_\n",
    "gs_results['RF'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'} \n",
      "best_score: 0.7554446460980035\n"
     ]
    }
   ],
   "source": [
    "# make Logistic Regressor\n",
    "clf_lr = LogisticRegression(random_state=1)\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2'], # type of regularisation \n",
    "    'C': [0.1, 1, 10, 100], # regularisation strength\n",
    "    'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg'] # approach to finding best weights\n",
    "}\n",
    "# so it doesn't redo lengthy GS\n",
    "params = {\n",
    "    'penalty': ['l2'], # type of regularisation \n",
    "    'C': [0.1], # regularisation strength\n",
    "    'solver': ['newton-cg'] # approach to finding best weights\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_lr, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "gs_results['LR'] = {'accuracy':best_score, 'params':best_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'} \n",
      "best_score: 0.7554446460980035\n"
     ]
    }
   ],
   "source": [
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_lr = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'var_smoothing': 1e-15} \n",
      "best_score: 0.6276769509981851\n"
     ]
    }
   ],
   "source": [
    "# make Gaussian Naive Bayes classifier\n",
    "clf_nb = GaussianNB()\n",
    "params = {\n",
    "    'var_smoothing':[1e-15, 1e-14, 1e-13, 1e-12, 1e-11, 1e-10, 1e-9], # from less smoothing to more aggressive smoothing\n",
    "}\n",
    "grid_search = GridSearchCV(clf_nb, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_nb = grid_search.best_estimator_\n",
    "gs_results['GNB'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'n_neighbors': 2} \n",
      "best_score: 0.8427102238354507\n"
     ]
    }
   ],
   "source": [
    "# make k-Nearest Neighbours classifier\n",
    "clf_knn = KNeighborsClassifier(n_jobs=-1) # use all processes for parellelisation\n",
    "params = {\n",
    "    'n_neighbors': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "}\n",
    "grid_search = GridSearchCV(clf_knn, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_knn = grid_search.best_estimator_\n",
    "gs_results['kNN'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'C': 1000, 'gamma': 'scale'} \n",
      "best_score: 0.8479431336963097\n"
     ]
    }
   ],
   "source": [
    "# make SVM-RBF classifier\n",
    "clf_svmrbf = SVC(kernel='rbf', random_state=1)\n",
    "params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000], # high to low regularisation strength\n",
    "    'gamma' : ['scale', 'auto'], # need to research this parameter more\n",
    "}\n",
    "\n",
    "# so it doesn't redo lengthy GS\n",
    "params = {\n",
    "    'C': [1000], # high to low regularisation strength\n",
    "    'gamma' : ['scale'], # need to research this parameter more\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_svmrbf, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_svmrbf = grid_search.best_estimator_\n",
    "gs_results['SVM-RBF'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'C': 1, 'gamma': 'scale'} \n",
      "best_score: 0.7623411978221415\n"
     ]
    }
   ],
   "source": [
    "# make SVM linear classifier\n",
    "clf_lin = SVC(kernel='linear', random_state=1)\n",
    "params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000], # high to low regularisation strength\n",
    "    'gamma' : ['scale', 'auto'], # need to research this parameter more\n",
    "    # 'gamma' : [], # need to research this parameter more\n",
    "}\n",
    "\n",
    "# so it doesn't redo lengthy GS\n",
    "params = {\n",
    "    'C': [1], # high to low regularisation strength\n",
    "    'gamma' : ['scale'], # need to research this parameter more\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(clf_lin, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_svmlin = grid_search.best_estimator_\n",
    "gs_results['SVM-Lin'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'C': 1e-05, 'gamma': 'scale'} \n",
      "best_score: 0.4493042952208105\n"
     ]
    }
   ],
   "source": [
    "# make svm sigmoidal classifier\n",
    "clf_sig = SVC(kernel='sigmoid', random_state=1)\n",
    "params = {\n",
    "    'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], # high to low regularisation strength\n",
    "    'gamma' : ['scale', 'auto'], # need to research this parameter more\n",
    "    # 'gamma' : [], # need to research this parameter more\n",
    "}\n",
    "\n",
    "# so it doesn't redo lengthy GS\n",
    "params = {\n",
    "    'C': [1e-05], # high to low regularisation strength\n",
    "    'gamma' : ['scale'], # need to research this parameter more\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_sig, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_svmsig = grid_search.best_estimator_\n",
    "gs_results['SVM-Sig'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'max_depth': 5, 'n_estimators': 10} \n",
      "best_score: 0.8287961282516637\n"
     ]
    }
   ],
   "source": [
    "# make xgboost classifier\n",
    "clf_xgb = xgb.XGBClassifier(random_state = 1)\n",
    "\n",
    "# encode labels, using sklearn, to pass to xgboost\n",
    "# this code was inspired by the snippet from:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "le = LabelEncoder()\n",
    "# fit the classes to the encoder and transform labels\n",
    "# e_train_labels = le.fit_transform(training_labels[0].to_list())\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [10,100, 500, 1000], # no. boosting rounds\n",
    "    'max_depth': [3,5,7,10,15] # control overfitting\n",
    "}\n",
    "\n",
    "# so it doesn't redo lengthy GS\n",
    "params = {\n",
    "    'n_estimators': [10], # no. boosting rounds\n",
    "    'max_depth': [5] # control overfitting\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_xgb, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, training_labels)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_xgb = grid_search.best_estimator_\n",
    "gs_results['XGB'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'learning_rate': 0.01, 'n_estimators': 10} \n",
      "best_score: 0.6696309739866908\n"
     ]
    }
   ],
   "source": [
    "# make adaboost classifier\n",
    "clf_ada = AdaBoostClassifier(random_state=1)\n",
    "params = {\n",
    "    'n_estimators': [10, 50, 100, 500, 1000],\n",
    "    'learning_rate': [0, 0.01, 0.1, 1, 10] # weight applied to each clf at each boosting iteration\n",
    "}\n",
    "\n",
    "# so it doesn't redo lengthy GS\n",
    "params = {\n",
    "    'n_estimators': [10],\n",
    "    'learning_rate': [0.01] # weight applied to each clf at each boosting iteration\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf_ada, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, training_labels)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"best_params: {best_params} \\nbest_score: {best_score}\")\n",
    "best_ada = grid_search.best_estimator_\n",
    "gs_results['ADA'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CART': {'accuracy': 0.8076527525710828, 'params': {'max_depth': 5, 'max_features': None}}, 'RF': {'accuracy': 0.8514216575922564, 'params': {'max_depth': None, 'max_features': 5, 'n_estimators': 200}}, 'LR': {'accuracy': 0.7554446460980035, 'params': {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}}, 'GNB': {'accuracy': 0.6276769509981851, 'params': {'var_smoothing': 1e-15}}, 'kNN': {'accuracy': 0.8427102238354507, 'params': {'n_neighbors': 2}}, 'SVM-RBF': {'accuracy': 0.8479431336963097, 'params': {'C': 1000, 'gamma': 'scale'}}, 'SVM-Lin': {'accuracy': 0.7623411978221415, 'params': {'C': 1, 'gamma': 'scale'}}, 'SVM-Sig': {'accuracy': 0.4493042952208105, 'params': {'C': 1e-05, 'gamma': 'scale'}}, 'XGB': {'accuracy': 0.8287961282516637, 'params': {'max_depth': 5, 'n_estimators': 10}}, 'ADA': {'accuracy': 0.6696309739866908, 'params': {'learning_rate': 0.01, 'n_estimators': 10}}}\n",
      "dict_keys(['RF', 'SVM-RBF', 'kNN', 'XGB', 'CART', 'SVM-Lin', 'LR', 'ADA', 'GNB', 'SVM-Sig'])\n"
     ]
    }
   ],
   "source": [
    "# sorted GS models\n",
    "print(gs_results)\n",
    "gs_sorted_models = dict(sorted(gs_results.items(), key=lambda item: item[1]['accuracy'], reverse=True))\n",
    "print(gs_sorted_models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  2  3]\n",
      " [ 0 57  6]\n",
      " [ 1 23 34]]\n",
      "[[21  1  0]\n",
      " [ 0 60  3]\n",
      " [ 1 20 37]]\n",
      "[[21  1  0]\n",
      " [ 1 50 12]\n",
      " [ 2 23 33]]\n",
      "[[21  1  0]\n",
      " [ 9 28 26]\n",
      " [ 3 13 42]]\n",
      "[[21  1  0]\n",
      " [ 0 61  2]\n",
      " [ 2 21 35]]\n",
      "[[20  0  2]\n",
      " [ 1 59  3]\n",
      " [ 3 17 38]]\n",
      "can't predict class probilities for SVM-RBF\n",
      "[[22  0  0]\n",
      " [ 1 44 18]\n",
      " [ 2 21 35]]\n",
      "can't predict class probilities for SVM-Lin\n",
      "[[ 0 22  0]\n",
      " [ 0 63  0]\n",
      " [ 0 58  0]]\n",
      "can't predict class probilities for SVM-Sig\n",
      "[[18  1  3]\n",
      " [ 0 62  1]\n",
      " [ 1 19 38]]\n",
      "[[20  2  0]\n",
      " [ 0 63  0]\n",
      " [ 1 57  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'GNB': {'accuracy': 0.6363636363636364,\n",
       "  'recall': {1: 0.9545454545454546,\n",
       "   2: 0.4444444444444444,\n",
       "   3: 0.7241379310344828},\n",
       "  'precision': {1: 0.6363636363636364,\n",
       "   2: 0.6666666666666666,\n",
       "   3: 0.6176470588235294},\n",
       "  'f1_score': {1: 0.7636363636363637,\n",
       "   2: 0.5333333333333333,\n",
       "   3: 0.6666666666666667},\n",
       "  'ROC-AUC': {1: 0.9521036814425244, 2: 0.7625, 3: 0.7460446247464503}},\n",
       " 'SVM-RBF': {'accuracy': 0.8181818181818182,\n",
       "  'recall': {1: 0.9090909090909091,\n",
       "   2: 0.9365079365079365,\n",
       "   3: 0.6551724137931034},\n",
       "  'precision': {1: 0.8333333333333334,\n",
       "   2: 0.7763157894736842,\n",
       "   3: 0.8837209302325582},\n",
       "  'f1_score': {1: 0.8695652173913043,\n",
       "   2: 0.8489208633093525,\n",
       "   3: 0.7524752475247525}},\n",
       " 'XGB': {'accuracy': 0.8251748251748252,\n",
       "  'recall': {1: 0.8181818181818182,\n",
       "   2: 0.9841269841269841,\n",
       "   3: 0.6551724137931034},\n",
       "  'precision': {1: 0.9473684210526315,\n",
       "   2: 0.7560975609756098,\n",
       "   3: 0.9047619047619048},\n",
       "  'f1_score': {1: 0.8780487804878049,\n",
       "   2: 0.8551724137931035,\n",
       "   3: 0.7599999999999999},\n",
       "  'ROC-AUC': {1: 0.9981217129977461,\n",
       "   2: 0.9221230158730158,\n",
       "   3: 0.8996957403651116}},\n",
       " 'RF': {'accuracy': 0.8251748251748252,\n",
       "  'recall': {1: 0.9545454545454546,\n",
       "   2: 0.9523809523809523,\n",
       "   3: 0.6379310344827587},\n",
       "  'precision': {1: 0.9545454545454546, 2: 0.7407407407407407, 3: 0.925},\n",
       "  'f1_score': {1: 0.9545454545454546,\n",
       "   2: 0.8333333333333334,\n",
       "   3: 0.7551020408163266},\n",
       "  'ROC-AUC': {1: 0.9996243425995491,\n",
       "   2: 0.9155753968253968,\n",
       "   3: 0.9144016227180527}},\n",
       " 'kNN': {'accuracy': 0.8181818181818182,\n",
       "  'recall': {1: 0.9545454545454546,\n",
       "   2: 0.9682539682539683,\n",
       "   3: 0.603448275862069},\n",
       "  'precision': {1: 0.9130434782608695,\n",
       "   2: 0.7349397590361446,\n",
       "   3: 0.9459459459459459},\n",
       "  'f1_score': {1: 0.9333333333333332,\n",
       "   2: 0.8356164383561644,\n",
       "   3: 0.7368421052631577},\n",
       "  'ROC-AUC': {1: 0.9765214124718258,\n",
       "   2: 0.8688492063492064,\n",
       "   3: 0.8438133874239351}},\n",
       " 'SVM-Lin': {'accuracy': 0.7062937062937062,\n",
       "  'recall': {1: 1.0, 2: 0.6984126984126984, 3: 0.603448275862069},\n",
       "  'precision': {1: 0.88, 2: 0.676923076923077, 3: 0.660377358490566},\n",
       "  'f1_score': {1: 0.9361702127659575, 2: 0.6875, 3: 0.6306306306306305}},\n",
       " 'CART': {'accuracy': 0.7552447552447552,\n",
       "  'recall': {1: 0.7727272727272727,\n",
       "   2: 0.9047619047619048,\n",
       "   3: 0.5862068965517241},\n",
       "  'precision': {1: 0.9444444444444444,\n",
       "   2: 0.6951219512195121,\n",
       "   3: 0.7906976744186046},\n",
       "  'f1_score': {1: 0.85, 2: 0.786206896551724, 3: 0.6732673267326732},\n",
       "  'ROC-AUC': {1: 0.899511645379414,\n",
       "   2: 0.8708333333333332,\n",
       "   3: 0.8319472616632861}},\n",
       " 'LR': {'accuracy': 0.7272727272727273,\n",
       "  'recall': {1: 0.9545454545454546,\n",
       "   2: 0.7936507936507936,\n",
       "   3: 0.5689655172413793},\n",
       "  'precision': {1: 0.875, 2: 0.6756756756756757, 3: 0.7333333333333333},\n",
       "  'f1_score': {1: 0.9130434782608695,\n",
       "   2: 0.72992700729927,\n",
       "   3: 0.6407766990291263},\n",
       "  'ROC-AUC': {1: 0.996619083395943,\n",
       "   2: 0.8212301587301587,\n",
       "   3: 0.8087221095334686}},\n",
       " 'SVM-Sig': {'accuracy': 0.4405594405594406,\n",
       "  'recall': {1: 0.0, 2: 1.0, 3: 0.0},\n",
       "  'precision': {1: 0.0, 2: 0.4405594405594406, 3: 0.0},\n",
       "  'f1_score': {1: 0.0, 2: 0.6116504854368932, 3: 0.0}},\n",
       " 'ADA': {'accuracy': 0.5804195804195804,\n",
       "  'recall': {1: 0.9090909090909091, 2: 1.0, 3: 0.0},\n",
       "  'precision': {1: 0.9523809523809523, 2: 0.5163934426229508, 3: 0.0},\n",
       "  'f1_score': {1: 0.9302325581395349, 2: 0.6810810810810811, 3: 0.0},\n",
       "  'ROC-AUC': {1: 0.9504132231404959, 2: 0.63125, 3: 0.6090263691683571}}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate models (Anon, 2023b)\n",
    "# model_metrics = {'accuracy', 'recall', 'precision', 'F1-score', 'ROC-AUC'}\n",
    "model_metrics = {}\n",
    "\n",
    "# all of the models\n",
    "models = [best_cart, best_rf, best_lr, best_nb, best_knn, best_svmrbf, best_svmlin, best_svmsig, best_xgb, best_ada]\n",
    "model_names = ['CART', 'RF', 'LR', 'GNB', 'kNN', 'SVM-RBF', 'SVM-Lin', 'SVM-Sig', 'XGB', 'ADA']\n",
    "i=0\n",
    "for model in models:\n",
    "    # train on test set\n",
    "    predicted = model.predict(testing_data)\n",
    "    # generate cm against test labels\n",
    "    cm = confusion_matrix(testing_labels, predicted)\n",
    "    print(cm)\n",
    "    accuracy = accuracy_score(testing_labels, predicted)\n",
    "    recall = recall_score(testing_labels, predicted, average=None)\n",
    "    precision = precision_score(testing_labels, predicted, average=None)\n",
    "    f1 = f1_score(testing_labels, predicted, average=None)\n",
    "\n",
    "    try:\n",
    "        predicted_prob = model.predict_proba(testing_data)\n",
    "        roc = roc_auc_score(testing_labels, predicted_prob, average=None, multi_class='ovr') \n",
    "        # print(accuracy, recall, precision, f1, roc)\n",
    "        model_metrics[model_names[i]] = {\n",
    "                                            'accuracy':accuracy, \n",
    "                                            'recall':{\n",
    "                                                1:recall[0], \n",
    "                                                2:recall[1], \n",
    "                                                3:recall[2]\n",
    "                                            },\n",
    "                                            'precision':{\n",
    "                                                1:precision[0], \n",
    "                                                2:precision[1], \n",
    "                                                3:precision[2]\n",
    "                                            },\n",
    "                                            'f1_score':{\n",
    "                                                1:f1[0], \n",
    "                                                2:f1[1], \n",
    "                                                3:f1[2]\n",
    "                                            },\n",
    "                                            'ROC-AUC':{\n",
    "                                                1:roc[0], \n",
    "                                                2:roc[1], \n",
    "                                                3:roc[2]\n",
    "                                            }\n",
    "        }\n",
    "    except:\n",
    "        print(f\"can't predict class probilities for {model_names[i]}\")\n",
    "        # print(accuracy, recall, precision, f1)\n",
    "        model_metrics[model_names[i]] = {\n",
    "                                            'accuracy':accuracy, \n",
    "                                            'recall':{\n",
    "                                                1:recall[0], \n",
    "                                                2:recall[1], \n",
    "                                                3:recall[2]\n",
    "                                            },\n",
    "                                            'precision':{\n",
    "                                                1:precision[0], \n",
    "                                                2:precision[1], \n",
    "                                                3:precision[2]\n",
    "                                            },\n",
    "                                            'f1_score':{\n",
    "                                                1:f1[0], \n",
    "                                                2:f1[1], \n",
    "                                                3:f1[2]\n",
    "                                            }\n",
    "        }\n",
    "    i+=1\n",
    "\n",
    "# (Gern Blanston, 2009)- sort by neoplasia recall\n",
    "sorted_metrics = dict(sorted(model_metrics.items(), key=lambda item: item[1]['recall'][3], reverse=True))\n",
    "# (holys, 2013)\n",
    "with open('metrics/selected_features/normalised_model_metrics_recall.json', 'w') as fp:\n",
    "    json.dump(sorted_metrics, fp)\n",
    "\n",
    "# redo but sort by accuracy\n",
    "# (Gern Blanston, 2009)\n",
    "sorted_metrics_acc = dict(sorted(model_metrics.items(), key=lambda item: item[1]['accuracy'], reverse=True))\n",
    "# (holys, 2013)\n",
    "with open('metrics/selected_features/normalised_model_metrics_accuracy.json', 'w') as fp:\n",
    "    json.dump(sorted_metrics_acc, fp)\n",
    "model_metrics\n",
    "sorted_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs_sorted_models (acc): \n",
      "dict_keys(['RF', 'SVM-RBF', 'kNN', 'XGB', 'CART', 'SVM-Lin', 'LR', 'ADA', 'GNB', 'SVM-Sig'])\n",
      "\n",
      "sorted models (acc): \n",
      "dict_keys(['RF', 'XGB', 'kNN', 'SVM-RBF', 'CART', 'LR', 'SVM-Lin', 'GNB', 'ADA', 'SVM-Sig'])\n",
      "\n",
      "sorted models (recall): \n",
      "dict_keys(['GNB', 'SVM-RBF', 'XGB', 'RF', 'kNN', 'SVM-Lin', 'CART', 'LR', 'SVM-Sig', 'ADA'])\n"
     ]
    }
   ],
   "source": [
    "# print highest acc models from gridsearch\n",
    "print(f\"gs_sorted_models (acc): \\n{gs_sorted_models.keys()}\\n\")\n",
    "\n",
    "# highest acc models from test set\n",
    "print(f\"sorted models (acc): \\n{sorted_metrics_acc.keys()}\\n\")\n",
    "\n",
    "# highest recall from test set\n",
    "print(f\"sorted models (recall): \\n{sorted_metrics.keys()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# observations on raw dataset\n",
    "XGBoost and ADAboost seem to have really overfit, because they severely underperform on unseen test data, compared to the accuracies they were achieving with gridsearch. IGNORE THIS: it is just because the test labels were not normalised!\n",
    "\n",
    "Although GNB has higher recall for neoplasia than kNN, kNN seems to be the best classifier overall. While GNB has highest recall for neoplasia, has 3rd lowest accuracy.\n",
    "\n",
    "Top models based on accuracy, from gridsearch, were RF, kNN, XGB, SVM-RBF, CART. Top models based on accuracy, from test set, were RF, kNN, SVM-RBF, CART, SVM-Lin. Therefore, RF, kNN, SVM-RBF, CART seem to perform well, in terms of accuracy, and don't seem to produce drastically different results with the test set, suggesting there isn't much overfitting\n",
    "\n",
    "# observations on feature selected dataset\n",
    "Some models decreased in performance, some increased, with largest increase being 6% increase in accuracy for SVM-Lin model. But overall, not worth, since the max accuracy of any of the models was lower than without feature selection. Maybe better feature selection is needed - an analytical solution rather than eyeball\n",
    "\n",
    "gs_sorted_models (acc): \n",
    "(['RF', 'SVM-RBF', 'kNN', 'XGB', 'CART', 'SVM-Lin', 'LR', 'ADA', 'GNB', 'SVM-Sig'])\n",
    "\n",
    "sorted models (acc): \n",
    "(['RF', 'XGB', 'kNN', 'SVM-RBF', 'CART', 'LR', 'SVM-Lin', 'GNB', 'ADA', 'SVM-Sig'])\n",
    "\n",
    "sorted models (recall): \n",
    "(['GNB', 'SVM-RBF', 'XGB', 'RF', 'kNN', 'SVM-Lin', 'CART', 'LR', 'SVM-Sig', 'ADA'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69968d942e1dcc7d0770d34dbcb974701730c09224194d51fbd302d9296a213d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
