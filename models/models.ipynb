{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import os, json, timeit\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score, confusion_matrix, accuracy_score, f1_score, precision_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from skopt import BayesSearchCV, Optimizer\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'augmentedv3_FS'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toggle option\n",
    "SNV = False\n",
    "FEATURE_SELECT = True\n",
    "FEATURE_SELECTv2 = False\n",
    "BALANCED = False\n",
    "MY_BALANCED = False\n",
    "MY_BALANCEDv2 = False\n",
    "AUGMENTED = False # oversampling of neoplasia\n",
    "AUGMENTEDv2 = False # actual augmented\n",
    "AUGMENTEDv3 = True # augmented neoplasia only\n",
    "SMOTE = False\n",
    "SVMSMOTE = False\n",
    "KMEANSSMOTE = False\n",
    "ADASYNSMOTE = False\n",
    "BORDERSMOTE = False\n",
    "TEST = False\n",
    "\n",
    "# Choose dataset\n",
    "DATASET = 'raw'\n",
    "FILENAME_ACC = 'metrics/raw_dataset/model_metrics_accuracy_ensemble.json'\n",
    "FILENAME_RECALL = 'metrics/raw_dataset/model_metrics_recall_ensemble.json'\n",
    "if TEST:\n",
    "    DATASET = 'test'\n",
    "    FILENAME_ACC = 'metrics/temp/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/temp/model_metrics_recall_ensemble.json'\n",
    "elif BALANCED:\n",
    "    DATASET = 'balanced'\n",
    "    FILENAME_ACC = 'metrics/balanced_dataset/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/balanced_dataset/model_metrics_recall_ensemble.json'\n",
    "    if SNV:\n",
    "        DATASET = 'snv_balanced'\n",
    "        FILENAME_ACC = 'metrics/balanced_dataset/model_metrics_accuracy_ensemble_snv.json'\n",
    "        FILENAME_RECALL = 'metrics/balanced_dataset/model_metrics_recall_ensemble_snv.json'\n",
    "        if FEATURE_SELECT:\n",
    "            DATASET = 'snv_FS_balanced'\n",
    "            FILENAME_ACC = 'metrics/balanced_dataset/model_metrics_accuracy_ensemble_snv_FS.json'\n",
    "            FILENAME_RECALL = 'metrics/balanced_dataset/model_metrics_recall_ensemble_snv_FS.json'\n",
    "elif MY_BALANCED:\n",
    "    DATASET = 'my_balanced'\n",
    "    FILENAME_ACC = 'metrics/my_balanced_dataset/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/my_balanced_dataset/model_metrics_recall_ensemble.json'\n",
    "    if FEATURE_SELECT:\n",
    "        DATASET = 'FS_my_balanced'\n",
    "        FILENAME_ACC = 'metrics/my_balanced_dataset/model_metrics_accuracy_ensemble_FS.json'\n",
    "        FILENAME_RECALL = 'metrics/my_balanced_dataset/model_metrics_recall_ensemble_FS.json'\n",
    "        if SNV:\n",
    "            DATASET = 'snv_FS_my_balanced'\n",
    "            FILENAME_ACC = 'metrics/my_balanced_dataset/model_metrics_accuracy_ensemble_snv_FS.json'\n",
    "            FILENAME_RECALL = 'metrics/my_balanced_dataset/model_metrics_recall_ensemble_snv_FS.json'\n",
    "    elif FEATURE_SELECTv2:\n",
    "        DATASET = 'FSv2_my_balanced'\n",
    "        FILENAME_ACC = 'metrics/my_balanced_dataset/model_metrics_accuracy_ensemble_FSv2.json'\n",
    "        FILENAME_RECALL = 'metrics/my_balanced_dataset/model_metrics_recall_ensemble_FSv2.json'\n",
    "    elif SNV:\n",
    "        DATASET = 'snv_my_balanced'\n",
    "        FILENAME_ACC = 'metrics/my_balanced_dataset/model_metrics_accuracy_ensemble_snv.json'\n",
    "        FILENAME_RECALL = 'metrics/my_balanced_dataset/model_metrics_recall_ensemble_snv.json'\n",
    "elif AUGMENTED:\n",
    "    DATASET = 'augmented'\n",
    "    FILENAME_ACC = 'metrics/augmented_dataset/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/augmented_dataset/model_metrics_recall_ensemble.json'\n",
    "    if FEATURE_SELECT:\n",
    "        DATASET = 'augmented_FS'\n",
    "        FILENAME_ACC = 'metrics/augmented_dataset/model_metrics_accuracy_ensemble_FS.json'\n",
    "        FILENAME_RECALL = 'metrics/augmented_dataset/model_metrics_recall_ensemble_FS.json'\n",
    "elif AUGMENTEDv2:\n",
    "    DATASET = 'augmentedv2'\n",
    "    FILENAME_ACC = 'metrics/augmentedv2_dataset/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/augmentedv2_dataset/model_metrics_recall_ensemble.json'\n",
    "    if FEATURE_SELECT:\n",
    "        DATASET = 'augmentedv2_FS'\n",
    "        FILENAME_ACC = 'metrics/augmentedv2_dataset/model_metrics_accuracy_ensemble_FS.json'\n",
    "        FILENAME_RECALL = 'metrics/augmentedv2_dataset/model_metrics_recall_ensemble_FS.json'\n",
    "elif AUGMENTEDv3:\n",
    "    DATASET = 'augmentedv3'\n",
    "    FILENAME_ACC = 'metrics/augmentedv3_dataset/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/augmentedv3_dataset/model_metrics_recall_ensemble.json'\n",
    "    if SNV:\n",
    "        DATASET = 'snv_augmentedv3'\n",
    "        FILENAME_ACC = 'metrics/augmentedv3_dataset/model_metrics_accuracy_ensemble_snv.json'\n",
    "        FILENAME_RECALL = 'metrics/augmentedv3_dataset/model_metrics_recall_ensemble_snv.json'\n",
    "    if FEATURE_SELECT:\n",
    "        DATASET = 'augmentedv3_FS'\n",
    "        FILENAME_ACC = 'metrics/augmentedv3_dataset/model_metrics_accuracy_ensemble_FS.json'\n",
    "        FILENAME_RECALL = 'metrics/augmentedv3_dataset/model_metrics_recall_ensemble_FS.json'\n",
    "elif SMOTE:\n",
    "    DATASET = 'smote'\n",
    "    FILENAME_ACC = 'metrics/smote/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/smote/model_metrics_recall_ensemble.json'\n",
    "elif SVMSMOTE:\n",
    "    DATASET = 'svmsmote'\n",
    "    FILENAME_ACC = 'metrics/svmsmote/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/svmsmote/model_metrics_recall_ensemble.json'\n",
    "    if FEATURE_SELECT:\n",
    "        DATASET = 'FS_svmsmote'\n",
    "        FILENAME_ACC = 'metrics/svmsmote/model_metrics_accuracy_ensemble_FS.json'\n",
    "        FILENAME_RECALL = 'metrics/svmsmote/model_metrics_recall_ensemble_FS.json'\n",
    "        if SNV:\n",
    "            DATASET = 'snv_FS_svmsmote'\n",
    "            FILENAME_ACC = 'metrics/svmsmote/model_metrics_accuracy_ensemble_snv_FS.json'\n",
    "            FILENAME_RECALL = 'metrics/svmsmote/model_metrics_recall_ensemble_snv_FS.json'\n",
    "    if SNV:\n",
    "            DATASET = 'snv_svmsmote'\n",
    "            FILENAME_ACC = 'metrics/svmsmote/model_metrics_accuracy_ensemble_FS.json'\n",
    "            FILENAME_RECALL = 'metrics/svmsmote/model_metrics_recall_ensemble_FS.json'\n",
    "elif KMEANSSMOTE:\n",
    "    DATASET = 'kmeanssmote'\n",
    "    FILENAME_ACC = 'metrics/kmeanssmote/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/kmeanssmote/model_metrics_recall_ensemble.json'\n",
    "elif ADASYNSMOTE:\n",
    "    DATASET = 'adasynsmote'\n",
    "    FILENAME_ACC = 'metrics/adasynsmote/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/adasynsmote/model_metrics_recall_ensemble.json'\n",
    "elif BORDERSMOTE:\n",
    "    DATASET = 'bordersmote'\n",
    "    FILENAME_ACC = 'metrics/bordersmote/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/bordersmote/model_metrics_recall_ensemble.json'\n",
    "elif MY_BALANCEDv2:\n",
    "    DATASET = 'my_balancedv2'\n",
    "    FILENAME_ACC = 'metrics/my_balancedv2_dataset/model_metrics_accuracy_ensemble.json'\n",
    "    FILENAME_RECALL = 'metrics/my_balancedv2_dataset/model_metrics_recall_ensemble.json'\n",
    "elif SNV:\n",
    "    DATASET = 'snv_raw'\n",
    "    FILENAME_ACC = 'metrics/raw_dataset/model_metrics_accuracy_snv.json'\n",
    "    FILENAME_RECALL = 'metrics/raw_dataset/model_metrics_recall_snv.json'\n",
    "elif FEATURE_SELECT:\n",
    "    DATASET = 'feature_select'\n",
    "    FILENAME_ACC = 'metrics/selected_features/model_metrics_accuracy.json'\n",
    "    FILENAME_RECALL = 'metrics/selected_features/model_metrics_recall.json'\n",
    "elif FEATURE_SELECTv2:\n",
    "    DATASET = 'feature_selectv2'\n",
    "    FILENAME_ACC = 'metrics/selected_features/model_metrics_accuracy2.json'\n",
    "    FILENAME_RECALL = 'metrics/selected_features/model_metrics_recall2.json'\n",
    "\n",
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmentedv3\n"
     ]
    }
   ],
   "source": [
    "if MY_BALANCED:\n",
    "    print('my_balanced')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/balanced_data/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/balanced_data/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/balanced_data/test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/balanced_data/test_label.csv', header = None)\n",
    "elif BALANCED:\n",
    "    print('balanced')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/original_data/balanced_train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/original_data/balanced_train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/balanced_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/balanced_test_label.csv', header = None)\n",
    "elif AUGMENTED:\n",
    "    print('augmented')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/augmented_data/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/augmented_data/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "elif AUGMENTEDv2:\n",
    "    print('augmentedv2')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/augmented_datav2/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/augmented_datav2/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "elif AUGMENTEDv3:\n",
    "    print('augmentedv3')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/augmented_datav3/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/augmented_datav3/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "elif MY_BALANCEDv2:\n",
    "    print('my_balancedv2')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/balancedv2_data/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/balancedv2_data/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "elif SMOTE:\n",
    "    print('smote')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/SMOTE/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/SMOTE/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "elif SVMSMOTE:\n",
    "    print('svmsmote')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/SMOTE/svm/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/SMOTE/svm/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "elif KMEANSSMOTE:\n",
    "    print('kmeanssmote')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/SMOTE/kmeans/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/SMOTE/kmeans/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "elif ADASYNSMOTE:\n",
    "    print('adasynsmote')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/SMOTE/adasyn/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/SMOTE/adasyn/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "elif BORDERSMOTE:\n",
    "    print('bordersmote')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/SMOTE/border/train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/SMOTE/border/train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "else:\n",
    "    print('raw')\n",
    "    # x_train\n",
    "    training_data = pd.read_csv('../data/original_data/noExclusion_train_data.csv', header = None)\n",
    "    # y_train\n",
    "    training_labels = pd.read_csv('../data/original_data/noExclusion_train_label.csv', header = None)\n",
    "    # x_test\n",
    "    testing_data = pd.read_csv('../data/original_data/noExclusion_test_data.csv', header = None)\n",
    "    # y_test\n",
    "    testing_labels = pd.read_csv('../data/original_data/noExclusion_test_label.csv', header = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data: <class 'pandas.core.frame.DataFrame'>, \n",
      "training_labels: <class 'pandas.core.frame.DataFrame'>, \n",
      "testing_data: <class 'pandas.core.frame.DataFrame'>, \n",
      "testing_labels: <class 'pandas.core.frame.DataFrame'>\n",
      "training labels vc: \n",
      "3    356\n",
      "2    257\n",
      "1    137\n",
      "dtype: int64, \n",
      "testing labels vc: \n",
      "2.0    63\n",
      "3.0    58\n",
      "1.0    22\n",
      "dtype: int64\n",
      "750 750 143 143\n"
     ]
    }
   ],
   "source": [
    "print(f\"training_data: {type(training_data)}, \\ntraining_labels: {type(training_labels)}, \\ntesting_data: {type(testing_data)}, \\ntesting_labels: {type(testing_labels)}\")\n",
    "print(f\"training labels vc: \\n{training_labels.value_counts()}, \\ntesting labels vc: \\n{testing_labels.value_counts()}\")\n",
    "print(len(training_data), len(training_labels), len(testing_data), len(testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type cast labels to ints\n",
    "training_labels[0] = training_labels[0].astype(int)\n",
    "# testing_labels\n",
    "testing_labels[0] = testing_labels[0].astype(int)\n",
    "\n",
    "# encode labels, using sklearn, to pass to xgboost\n",
    "# this code was inspired by the snippet from:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "le = LabelEncoder()\n",
    "# fit the classes to the encoder and transform labels to be 0,1,2\n",
    "training_labels = le.fit_transform(training_labels[0].to_list())\n",
    "testing_labels = le.fit_transform(testing_labels[0].to_list())\n",
    "\n",
    "# gridsearch results\n",
    "gs_results = {}\n",
    "np.unique(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS TO APPLY FEATURE SELECTION TO TRAINING DATA\n",
    "if FEATURE_SELECT == True:\n",
    "    ADD_POSSIBLE_FIGURES = True\n",
    "\n",
    "    # figures contain features from (figure_num*4)+1 \n",
    "    selected_figures = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\n",
    "                        21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,\n",
    "                        39,40,41,42,43,44,45,46,47,48,49,50,51,52]\n",
    "\n",
    "    # figures that MAY be decent - noisy but different peaks\n",
    "    possible_figures = [53,54,56,57,58,59,60,61,62,63,64,65]\n",
    "\n",
    "    # decent features - eyeballed\n",
    "\n",
    "    # generate set of selected features\n",
    "    selected_features = []\n",
    "\n",
    "    for figure_num in selected_figures:\n",
    "        for i in range(0,4):\n",
    "            # print(f\"figure: {figure_num}, feature: feature_{(figure_num*4)+i}\")\n",
    "            selected_features.append((figure_num*4)+i)\n",
    "\n",
    "    # to add possible features\n",
    "    if ADD_POSSIBLE_FIGURES == True:\n",
    "        for figure_num in possible_figures:\n",
    "            for i in range(0,4):\n",
    "                # print(f\"figure: {figure_num}, feature: feature_{(figure_num*4)+i}\")\n",
    "                selected_features.append((figure_num*4)+i)\n",
    "                \n",
    "    training_data = training_data[selected_features]\n",
    "    testing_data = testing_data[selected_features]\n",
    "else:\n",
    "    print('no feature selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no FSv2\n"
     ]
    }
   ],
   "source": [
    "if FEATURE_SELECTv2:    \n",
    "    # selected features round 2\n",
    "    selected_features = []\n",
    "    a = np.arange(30,85).tolist()\n",
    "    b = np.arange(203,235).tolist()\n",
    "\n",
    "    selected_features = np.concatenate([a,b]).tolist()\n",
    "    print(selected_features)\n",
    "    # (Cena, 2018)\n",
    "    training_data = training_data[selected_features]\n",
    "    testing_data = testing_data[selected_features]\n",
    "else:\n",
    "    print('no FSv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 260)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no SNV standardisation\n"
     ]
    }
   ],
   "source": [
    "# apply SNV to training data - inspired by code from my ML CW\n",
    "# (Hamzah Hafejee, 2022, COMP3611_Coursework_Assessment.ipynb, Comp 3611, University of Leeds)\n",
    "# (Sklearn, 2023)\n",
    "if SNV == True:\n",
    "    print(len(training_data))\n",
    "    # fit to training data\n",
    "    scaler = StandardScaler().fit(training_data)\n",
    "    training_data = scaler.transform(training_data)\n",
    "    testing_data = scaler.transform(testing_data)\n",
    "    print(\"After: \\n\", len(training_data))\n",
    "    len(training_data)\n",
    "else:\n",
    "    print('no SNV standardisation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'augmentedv3_FS'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO USE AVG RECALL AS METRIC FOR GS\n",
    "# (gunes, 2019)\n",
    "gs_recall = make_scorer(recall_score, average='macro')\n",
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'max_depth': None, 'max_features': 'log2'} \n",
      "best_score: 0.7853333333333333\n"
     ]
    }
   ],
   "source": [
    "# make CART classifier\n",
    "clf_cart = tree.DecisionTreeClassifier(criterion=\"gini\", random_state=1)\n",
    "# find optimal parameter values for CART\n",
    "params = {\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40], # control overfitting,\n",
    "    'max_features': [None, 'sqrt', 'log2'] # performance \n",
    "}\n",
    "\n",
    "if DATASET == 'raw':\n",
    "    # raw dataset\n",
    "    params = {\n",
    "        'max_depth': [None], # control overfitting,\n",
    "        'max_features': ['log2'] # performance \n",
    "    }\n",
    "elif DATASET == 'feature_select':\n",
    "    # selected + possible features\n",
    "    params = {\n",
    "        'max_depth': [5], # control overfitting,\n",
    "        'max_features': [None] # performance \n",
    "    }\n",
    "elif DATASET == 'feature_selectv2':\n",
    "    # feature select v2\n",
    "    params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "elif DATASET == 'snv_raw':\n",
    "    # SNV + raw\n",
    "    params = {'max_depth': [None], 'max_features': ['log2']} \n",
    "elif DATASET == 'balanced':\n",
    "    params = {'max_depth': [10], 'max_features': ['sqrt']} \n",
    "elif DATASET == 'snv_balanced':\n",
    "    params = {'max_depth': [10], 'max_features': ['sqrt']} \n",
    "\n",
    "elif DATASET == 'my_balanced':\n",
    "    params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "\n",
    "elif DATASET == 'FS_my_balanced':\n",
    "    params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "    \n",
    "elif DATASET == 'FSv2_my_balanced':\n",
    "    params = {'max_depth': [10], 'max_features': ['log2']} \n",
    "    \n",
    "elif DATASET == 'snv_my_balanced':\n",
    "    params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "\n",
    "elif DATASET == 'snv_FS_my_balanced':\n",
    "    params = {'max_depth': [5], 'max_features': [None]} \n",
    "   \n",
    "elif DATASET == 'augmented':\n",
    "    params = {'max_depth': [10], 'max_features': ['log2']} \n",
    "\n",
    "elif DATASET == 'augmented_FS':\n",
    "    params = {'max_depth': [10], 'max_features': ['sqrt']} \n",
    "\n",
    "elif DATASET == 'smote':\n",
    "    params = {'max_depth': [15], 'max_features': ['sqrt']} \n",
    "\n",
    "elif DATASET == 'augmentedv3':\n",
    "    params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "    \n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "    params = {'max_depth': [None], 'max_features': ['log2']}\n",
    " \n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "    params = {'max_depth': [10], 'max_features': [None]} \n",
    "\n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "    params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "\n",
    "elif DATASET == 'kmeanssmote':\n",
    "    params = {'max_depth': [10], 'max_features': ['sqrt']} \n",
    "    \n",
    "elif DATASET == 'svmsmote':\n",
    "    params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "    \n",
    "elif DATASET == 'adasynsmote':\n",
    "    params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "     \n",
    "elif DATASET == 'bordersmote':\n",
    "    params = {'max_depth': [None], 'max_features': ['sqrt']} \n",
    "     \n",
    "elif DATASET == 'snv_svmsmote':\n",
    "    params = {'max_depth': [None], 'max_features': ['log2']}\n",
    "    \n",
    "elif DATASET == 'snv_FS_svmsmote':\n",
    "    params = {'max_depth': [10], 'max_features': [None]} \n",
    "    \n",
    "grid_search = GridSearchCV(clf_cart, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_cart = grid_search.best_estimator_\n",
    "gs_results['CART'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bayes search experiment (Skopt, 2017)\n",
    "# clf_cart = tree.DecisionTreeClassifier(criterion=\"gini\", random_state=1)\n",
    "# # find optimal parameter values for CART\n",
    "# params = {\n",
    "#     'max_depth': Integer(1,100), # control overfitting,\n",
    "#     'max_features': Categorical([None, 'sqrt', 'log2']) # performance \n",
    "# }\n",
    "   \n",
    "# bayes_search = BayesSearchCV(clf_cart, params, scoring='accuracy', cv=10, random_state=1)\n",
    "# _ = bayes_search.fit(training_data, np.ravel(training_labels))\n",
    "# best_params = bayes_search.best_params_\n",
    "# best_score = bayes_search.best_score_\n",
    "# print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "# best_cart = bayes_search.best_estimator_\n",
    "# gs_results['CART'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'var_smoothing': 1e-20} \n",
      "best_score: 0.6626666666666666\n"
     ]
    }
   ],
   "source": [
    "# make Gaussian Naive Bayes classifier\n",
    "clf_nb = GaussianNB()\n",
    "params = {\n",
    "    'var_smoothing':[1e-20, 1e-19, 1e-18, 1e-17, 1e-16, 1e-15, 1e-14, 1e-13, 1e-12, 1e-11, 1e-10, 1e-9], # from less smoothing to more aggressive smoothing\n",
    "}\n",
    "grid_search = GridSearchCV(clf_nb, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_nb = grid_search.best_estimator_\n",
    "gs_results['GNB'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'n_neighbors': 1} \n",
      "best_score: 0.8186666666666665\n"
     ]
    }
   ],
   "source": [
    "# make k-Nearest Neighbours classifier\n",
    "clf_knn = KNeighborsClassifier(n_jobs=-1) # use all processes for parellelisation\n",
    "params = {\n",
    "    'n_neighbors': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "}\n",
    "grid_search = GridSearchCV(clf_knn, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_knn = grid_search.best_estimator_\n",
    "gs_results['kNN'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'C': 5000, 'gamma': 'scale'} \n",
      "best_score: 0.8800000000000001\n"
     ]
    }
   ],
   "source": [
    "# make SVM-RBF classifier\n",
    "clf_svmrbf = SVC(kernel='rbf', random_state=1)\n",
    "params = {\n",
    "    'C': [0.1, 1, 10, 100, 1000, 5000], # high to low regularisation strength\n",
    "    'gamma' : ['scale', 'auto'], # need to research this parameter more\n",
    "}\n",
    "\n",
    "if DATASET == 'raw':\n",
    "    # raw dataset\n",
    "    params = {\n",
    "        'C': [100], # high to low regularisation strength\n",
    "        'gamma' : ['scale'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_select':\n",
    "    # selected + possible features\n",
    "    params = {\n",
    "        'C': [1000], # high to low regularisation strength\n",
    "        'gamma' : ['scale'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_selectv2':\n",
    "    # selected + possible features v2\n",
    "    params = {\n",
    "        'C': [1000], # high to low regularisation strength\n",
    "        'gamma' : ['scale'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'snv_raw':\n",
    "    # SNV + raw\n",
    "    params = {'C': [10], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'balanced':\n",
    "    params = {'C': [1000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'snv_balanced':\n",
    "    params = {'C': [1000], 'gamma': ['scale']}\n",
    "\n",
    "elif DATASET == 'my_balanced':\n",
    "    params = {'C': [100], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'FS_my_balanced':\n",
    "    params = {'C': [100], 'gamma':[ 'scale']} \n",
    "\n",
    "elif DATASET == 'FSv2_my_balanced':\n",
    "    params = {'C': [5000], 'gamma':[ 'scale']} \n",
    "    \n",
    "elif DATASET == 'snv_my_balanced':\n",
    "    params = {'C': [100], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'snv_FS_my_balanced':\n",
    "    params = {'C': [5000], 'gamma':[ 'scale']} \n",
    "    \n",
    "elif DATASET == 'augmented':\n",
    "    params = {'C': [5000], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'augmented_FS':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'smote':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'augmentedv3':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'kmeanssmote':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'svmsmote':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'adasynsmote':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'bordersmote':\n",
    "    params = {'C': [5000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'snv_svmsmote':\n",
    "    params = {'C': [1000], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'snv_FS_svmsmote':\n",
    "    params = {'C': [5000], 'gamma': ['auto']} \n",
    "    \n",
    "grid_search = GridSearchCV(clf_svmrbf, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_svmrbf = grid_search.best_estimator_\n",
    "gs_results['SVM-RBF'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m     params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m100\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m:[ \u001b[39m'\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m'\u001b[39m]} \n\u001b[0;32m     94\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(clf_lin, params, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(training_data, np\u001b[39m.\u001b[39;49mravel(training_labels))\n\u001b[0;32m     96\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m     97\u001b[0m best_score \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_score_\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:252\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    251\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[1;32m--> 252\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[0;32m    253\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\sc19mhh\\Desktop\\Hamzah\\Uni\\CompSci\\ThirdYear\\FYP\\MyCode\\ML-cancer-detection\\.venv\\lib\\site-packages\\sklearn\\svm\\_base.py:331\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    317\u001b[0m libsvm\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    319\u001b[0m \u001b[39m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[39m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    321\u001b[0m (\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_,\n\u001b[0;32m    323\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_,\n\u001b[0;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support,\n\u001b[0;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual_coef_,\n\u001b[0;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_,\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA,\n\u001b[0;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB,\n\u001b[0;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_,\n\u001b[0;32m    330\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_iter,\n\u001b[1;32m--> 331\u001b[0m ) \u001b[39m=\u001b[39m libsvm\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    332\u001b[0m     X,\n\u001b[0;32m    333\u001b[0m     y,\n\u001b[0;32m    334\u001b[0m     svm_type\u001b[39m=\u001b[39;49msolver_type,\n\u001b[0;32m    335\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    336\u001b[0m     \u001b[39m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[0;32m    337\u001b[0m     class_weight\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_class_weight\u001b[39;49m\u001b[39m\"\u001b[39;49m, np\u001b[39m.\u001b[39;49mempty(\u001b[39m0\u001b[39;49m)),\n\u001b[0;32m    338\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[0;32m    339\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[0;32m    340\u001b[0m     nu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu,\n\u001b[0;32m    341\u001b[0m     probability\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability,\n\u001b[0;32m    342\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    343\u001b[0m     shrinking\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking,\n\u001b[0;32m    344\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    345\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    346\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    347\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    348\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[0;32m    349\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    350\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    351\u001b[0m )\n\u001b[0;32m    353\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make SVM linear classifier\n",
    "clf_lin = SVC(kernel='linear', random_state=1)\n",
    "params = {\n",
    "    'C': [0.05, 0.1, 1, 10, 100, 1000], # high to low regularisation strength\n",
    "    'gamma' : ['scale', 'auto'], # need to research this parameter more\n",
    "    # 'gamma' : [], # need to research this parameter more\n",
    "}\n",
    "\n",
    "if DATASET == 'raw':\n",
    "    # raw dataset\n",
    "    params = {\n",
    "        'C': [10], # high to low regularisation strength\n",
    "        'gamma' : ['scale'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_select':\n",
    "    # selected + possible features\n",
    "    params = {\n",
    "        'C': [1], # high to low regularisation strength\n",
    "        'gamma' : ['scale'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_selectv2':\n",
    "    # FSv2\n",
    "    params = {\n",
    "        'C': [1], # high to low regularisation strength\n",
    "        'gamma' : ['scale'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'snv_raw':\n",
    "    # SNV + raw\n",
    "    params = {'C': [0.1], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'balanced':\n",
    "    params = {'C': [1], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'snv_balanced':\n",
    "    params = {'C': [0.1], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'my_balanced':\n",
    "    params = {'C': [10], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'FS_my_balanced':\n",
    "    params = {'C': [10], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'FSv2_my_balanced':\n",
    "    params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "    \n",
    "elif DATASET == 'snv_my_balanced':\n",
    "    params = {'C': [0.05], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'snv_FS_my_balanced':\n",
    "    params = {'C': [0.1], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'augmented':\n",
    "    params = {'C': [10], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'augmented_FS':\n",
    "    params = {'C': [100], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'smote':\n",
    "    params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "    \n",
    "elif DATASET == 'augmentedv3':\n",
    "    params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "\n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "    params = {'C': [100], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "    params = {'C': [1], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "    params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "    \n",
    "elif DATASET == 'svmsmote':\n",
    "    params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "        \n",
    "elif DATASET == 'kmeanssmote':\n",
    "    params = {'C': [10], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'adasynsmote':\n",
    "    params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "    \n",
    "elif DATASET == 'bordersmote':\n",
    "    params = {'C': [1000], 'gamma':[ 'scale']} \n",
    "    \n",
    "elif DATASET == 'snv_svmsmote':\n",
    "    params = {'C': [100], 'gamma':[ 'scale']} \n",
    "    \n",
    "elif DATASET == 'snv_FS_svmsmote':\n",
    "    params = {'C': [100], 'gamma':[ 'scale']} \n",
    "    \n",
    "grid_search = GridSearchCV(clf_lin, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_svmlin = grid_search.best_estimator_\n",
    "gs_results['SVM-Lin'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'C': 100, 'gamma': 'auto'} \n",
      "best_score: 0.6266666666666667\n"
     ]
    }
   ],
   "source": [
    "# make svm sigmoidal classifier\n",
    "clf_sig = SVC(kernel='sigmoid', random_state=1)\n",
    "params = {\n",
    "    'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100], # high to low regularisation strength\n",
    "    'gamma' : ['scale', 'auto'], # need to research this parameter more\n",
    "    # 'gamma' : [], # need to research this parameter more\n",
    "}\n",
    "\n",
    "if DATASET == 'raw':\n",
    "    # raw dataset\n",
    "    params = {\n",
    "        'C': [10], # high to low regularisation strength\n",
    "        'gamma' : ['auto'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_select':\n",
    "    # selected + possible features\n",
    "    params = {\n",
    "        'C': [1e-05], # high to low regularisation strength\n",
    "        'gamma' : ['scale'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_selectv2':\n",
    "    # FSv2\n",
    "    params = {\n",
    "        'C': [0.1], # high to low regularisation strength\n",
    "        'gamma' : ['auto'], # need to research this parameter more\n",
    "    }\n",
    "\n",
    "elif DATASET == 'snv_raw':\n",
    "    # SNV + raw\n",
    "    params = {'C': [0.1], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'balanced':\n",
    "    params = {'C': [0.1], 'gamma': ['auto']}\n",
    "\n",
    "elif DATASET == 'snv_balanced':\n",
    "    params = {'C': [0.1], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'my_balanced':\n",
    "    params = {'C': [10], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'FS_my_balanced':\n",
    "    params = {'C': [10], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'FSv2_my_balanced':\n",
    "    params = {'C': [1], 'gamma':[ 'auto']} \n",
    "    \n",
    "elif DATASET == 'snv_my_balanced':\n",
    "    params = {'C': [0.1], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'snv_FS_my_balanced':\n",
    "    params = {'C': [0.1], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'augmented':\n",
    "    params = {'C': [100], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'augmented_FS':\n",
    "    params = {'C': [100], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'smote':\n",
    "    params = {'C': [0.1], 'gamma': ['auto']} \n",
    "    \n",
    "elif DATASET == 'augmentedv3':\n",
    "    params = {'C': [100], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "    params = {'C': [0.01], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "    params = {'C': [1], 'gamma': ['scale']} \n",
    "\n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "    params = {'C': [0.1], 'gamma': ['auto']} \n",
    "\n",
    "elif DATASET == 'svmsmote':\n",
    "    params = {'C': [0.1], 'gamma': ['auto']} \n",
    "    \n",
    "elif DATASET == 'kmeanssmote':\n",
    "    params = {'C': [100], 'gamma': ['auto']} \n",
    "    \n",
    "elif DATASET == 'adasynsmote':\n",
    "    params = {'C': [0.1], 'gamma': ['auto']} \n",
    "    \n",
    "elif DATASET == 'bordersmote':\n",
    "    params = {'C': [0.1], 'gamma': ['auto']} \n",
    "    \n",
    "elif DATASET == 'snv_svmsmote':\n",
    "    params = {'C': [0.1], 'gamma': ['scale']} \n",
    "    \n",
    "elif DATASET == 'snv_FS_svmsmote':\n",
    "    params = {'C': [0.1], 'gamma': ['scale']} \n",
    "    \n",
    "grid_search = GridSearchCV(clf_sig, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_svmsig = grid_search.best_estimator_\n",
    "gs_results['SVM-Sig'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'max_depth': 3, 'n_estimators': 100} \n",
      "best_score: 0.8480000000000001\n"
     ]
    }
   ],
   "source": [
    "# make xgboost classifier (Piotr Poski, 2021) (Jain, 2016)\n",
    "clf_xgb = xgb.XGBClassifier(random_state = 1)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [10,100, 500, 1000], # no. boosting rounds\n",
    "    'max_depth': [3,5,7,10,15,20] # control overfitting\n",
    "}\n",
    "\n",
    "if DATASET == 'raw':\n",
    "    # raw dataset\n",
    "    params = {\n",
    "        'n_estimators': [100], # no. boosting rounds\n",
    "        'max_depth': [10] # control overfitting\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_select':\n",
    "    # selected + possible features\n",
    "    params = {\n",
    "        'n_estimators': [10], # no. boosting rounds\n",
    "        'max_depth': [5] # control overfitting\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_selectv2':\n",
    "    # FSv2\n",
    "    params = {\n",
    "        'n_estimators': [100], # no. boosting rounds\n",
    "        'max_depth': [5] # control overfitting\n",
    "    }\n",
    "\n",
    "elif DATASET == 'snv_raw':\n",
    "    # SNV + raw\n",
    "    params = {'max_depth': [10], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'balanced':\n",
    "    params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'snv_balanced':\n",
    "    params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "    \n",
    "elif DATASET == 'my_balanced':\n",
    "    params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'FS_my_balanced':\n",
    "    params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'FSv2_my_balanced':\n",
    "    params = {'max_depth': [15], 'n_estimators': [500]} \n",
    "    \n",
    "elif DATASET == 'snv_my_balanced':\n",
    "    params = {'max_depth': [15], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'snv_FS_my_balanced':\n",
    "    params = {'max_depth': [10], 'n_estimators': [100]} \n",
    "    \n",
    "elif DATASET == 'augmented':\n",
    "    params = {'max_depth': [5], 'n_estimators': [500]} \n",
    "\n",
    "elif DATASET == 'augmented_FS':\n",
    "    params = {'max_depth': [10], 'n_estimators': [1000]} \n",
    "\n",
    "elif DATASET == 'smote':\n",
    "    params = {'max_depth': [7], 'n_estimators': [500]} \n",
    "\n",
    "elif DATASET == 'augmentedv3':\n",
    "    params = {'max_depth': [3], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "    params = {'max_depth': [3], 'n_estimators': [100]} \n",
    "       \n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "    params = {'max_depth': [5], 'n_estimators': [500]} \n",
    "\n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "    params = {'max_depth': [7], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'svmsmote':\n",
    "    params = {'max_depth': [5], 'n_estimators': [500]} \n",
    "    \n",
    "elif DATASET == 'kmeanssmote':\n",
    "    params = {'max_depth': [10], 'n_estimators': [500]} \n",
    "    \n",
    "elif DATASET == 'adasynsmote':\n",
    "    params = {'max_depth': [5], 'n_estimators': [1000]} \n",
    "    \n",
    "elif DATASET == 'bordersmote':\n",
    "    params = {'max_depth': [5], 'n_estimators': [100]} \n",
    "    \n",
    "elif DATASET == 'snv_svmsmote':\n",
    "    params = {'max_depth': [5], 'n_estimators': [500]} \n",
    "    \n",
    "elif DATASET == 'snv_FS_svmsmote':\n",
    "    params = {'max_depth': [3], 'n_estimators': [1000]} \n",
    "    \n",
    "grid_search = GridSearchCV(clf_xgb, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, training_labels)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_xgb = grid_search.best_estimator_\n",
    "gs_results['XGB'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'learning_rate': 0.001, 'n_estimators': 1000} \n",
      "best_score: 0.7133333333333333\n"
     ]
    }
   ],
   "source": [
    "# make adaboost classifier\n",
    "clf_ada = AdaBoostClassifier(random_state=1)\n",
    "params = {\n",
    "    'n_estimators': [10, 50, 100, 500, 1000],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1, 10] # weight applied to each clf at each boosting iteration\n",
    "}\n",
    "\n",
    "if DATASET == 'raw':\n",
    "    # raw dataset\n",
    "    params = {\n",
    "        'n_estimators': [50],\n",
    "        'learning_rate': [0.01] # weight applied to each clf at each boosting iteration\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_select':\n",
    "    # selected + possible features\n",
    "    params = {\n",
    "        'n_estimators': [10],\n",
    "        'learning_rate': [0.01] # weight applied to each clf at each boosting iteration\n",
    "    }\n",
    "\n",
    "elif DATASET == 'feature_selectv2':\n",
    "    # FSv2\n",
    "    params = {\n",
    "        'n_estimators': [50],\n",
    "        'learning_rate': [0.01] # weight applied to each clf at each boosting iteration\n",
    "    }\n",
    "\n",
    "elif DATASET == 'snv_raw':\n",
    "    # SNV + raw\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [50]} \n",
    "\n",
    "elif DATASET == 'balanced':\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [10]} \n",
    "\n",
    "elif DATASET == 'snv_balanced':\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [10]} \n",
    "    \n",
    "elif DATASET == 'my_balanced':\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'FS_my_balanced':\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'FSv2_my_balanced':\n",
    "    params = {'learning_rate': [0.1], 'n_estimators': [10]} \n",
    "    \n",
    "elif DATASET == 'snv_my_balanced':\n",
    "    params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "\n",
    "elif DATASET == 'snv_FS_my_balanced':\n",
    "    params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "    \n",
    "elif DATASET == 'augmented':\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [50]} \n",
    "\n",
    "elif DATASET == 'augmented_FS':\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [50]} \n",
    "\n",
    "elif DATASET == 'smote':\n",
    "    params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "    \n",
    "elif DATASET == 'augmentedv3':\n",
    "    params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "\n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "    params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "        \n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "    params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "\n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "    params = {'learning_rate': [0.001], 'n_estimators': [1000]} \n",
    "    \n",
    "elif DATASET == 'svmsmote':\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [1000]} \n",
    "    \n",
    "elif DATASET == 'kmeanssmote':\n",
    "    params = {'learning_rate': [0.1], 'n_estimators': [10]} \n",
    "    \n",
    "elif DATASET == 'adasynsmote':\n",
    "    params = {'learning_rate': [1], 'n_estimators': [500]} \n",
    "    \n",
    "elif DATASET == 'bordersmote':\n",
    "    params = {'learning_rate': [1], 'n_estimators': [500]} \n",
    "    \n",
    "elif DATASET == 'snv_svmsmote':\n",
    "    params = {'learning_rate': [0.01], 'n_estimators': [1000]} \n",
    "    \n",
    "elif DATASET == 'snv_FS_svmsmote':\n",
    "    params = {'learning_rate': [0.1], 'n_estimators': [10]} \n",
    "    \n",
    "grid_search = GridSearchCV(clf_ada, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, training_labels)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_ada = grid_search.best_estimator_\n",
    "gs_results['ADA'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmentedv3\n",
      "params = {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'} \n",
      "best_score: 0.796\n"
     ]
    }
   ],
   "source": [
    "# make Logistic Regressor\n",
    "clf_lr = LogisticRegression(random_state=1, max_iter=1000)\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2'], # type of regularisation \n",
    "    'C': [0.1, 1, 10, 100], # regularisation strength\n",
    "    'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg'] # approach to finding best weights\n",
    "}\n",
    "\n",
    "print(DATASET)\n",
    "\n",
    "if DATASET == 'raw':\n",
    "    # raw dataset\n",
    "    params = {\n",
    "        'penalty': ['l2'], # type of regularisation \n",
    "        'C': [0.1], # regularisation strength\n",
    "        'solver': ['lbfgs'] # approach to finding best weights\n",
    "    }\n",
    "elif DATASET == 'feature_select':\n",
    "    # selected + possible features\n",
    "    params = {\n",
    "        'penalty': ['l2'], # type of regularisation \n",
    "        'C': [0.1], # regularisation strength\n",
    "        'solver': ['newton-cg'] # approach to finding best weights\n",
    "    }\n",
    "elif DATASET == 'feature_selectv2':\n",
    "    # FSv2\n",
    "    params = {'C': [1], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "elif DATASET == 'snv_raw':\n",
    "    # SNV + raw\n",
    "    params = {'C': [0.1], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "elif DATASET == 'balanced':\n",
    "    params = {'C': [100], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "elif DATASET == 'snv_balanced':\n",
    "    params = {'C': [0.1], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "\n",
    "elif DATASET == 'my_balanced':\n",
    "    params = {'C': [10], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "    \n",
    "elif DATASET == 'FS_my_balanced':\n",
    "    params = {'C': [10], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "    \n",
    "elif DATASET == 'FSv2_my_balanced':\n",
    "    params = {'C': [10], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "    \n",
    "elif DATASET == 'snv_my_balanced':\n",
    "    params = {'C': [1], 'penalty': ['l1'], 'solver': ['saga']} \n",
    "    \n",
    "elif DATASET == 'snv_FS_my_balanced':\n",
    "    params = {'C': [1], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "    \n",
    "elif DATASET == 'augmented':\n",
    "    params = {'C': [10], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "    \n",
    "elif DATASET == 'augmented_FS':\n",
    "    params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "\n",
    "elif DATASET == 'smote':\n",
    "    params = {'C': [10], 'penalty': ['l2'], 'solver': ['saga']} \n",
    "\n",
    "elif DATASET == 'augmentedv3':\n",
    "    params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "    \n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "    params = {'C': [10], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "    \n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "    params = {'C': [10], 'penalty': ['l1'], 'solver': ['saga']} \n",
    "\n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "    params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "    \n",
    "elif DATASET == 'svmsmote':\n",
    "    params = {'C': [100], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "    \n",
    "elif DATASET == 'kmeanssmote':\n",
    "    params = {'C': [10], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "    \n",
    "elif DATASET == 'adasynsmote':\n",
    "    params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "    \n",
    "elif DATASET == 'bordersmote':\n",
    "    params = {'C': [100], 'penalty': ['l1'], 'solver': ['liblinear']} \n",
    "    \n",
    "elif DATASET == 'snv_svmsmote':\n",
    "    params = {'C': [100], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "    \n",
    "elif DATASET == 'snv_FS_svmsmote':\n",
    "    params = {'C': [100], 'penalty': ['l2'], 'solver': ['lbfgs']} \n",
    "    \n",
    "grid_search = GridSearchCV(clf_lr, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "gs_results['LR'] = {'accuracy':best_score, 'params':best_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'} \n",
      "best_score: 0.796\n"
     ]
    }
   ],
   "source": [
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_lr = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'max_depth': None, 'max_features': 10, 'n_estimators': 200} \n",
      "best_score: 0.8626666666666667\n"
     ]
    }
   ],
   "source": [
    "# make Random Forest classifier\n",
    "clf_rf = RandomForestClassifier(random_state=1)\n",
    "params = {\n",
    "  'n_estimators': [10, 50, 100, 200, 300],\n",
    "  'max_depth': [None, 5, 10, 20, 30, 40],\n",
    "  \"max_features\" : [None, 1, 5, 10, 20, 30]\n",
    "}\n",
    "\n",
    "if DATASET == 'raw':\n",
    "  # raw dataset\n",
    "  params = {\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [10],\n",
    "    \"max_features\" : [30]\n",
    "  }\n",
    "\n",
    "elif DATASET == 'feature_select':\n",
    "  # selected + possible features\n",
    "  params = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [None],\n",
    "    \"max_features\" : [5]\n",
    "  }\n",
    "\n",
    "elif DATASET == 'feature_selectv2':\n",
    "  # feature select v2\n",
    "  params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [300]}\n",
    "\n",
    "elif DATASET == 'snv_raw':\n",
    "  # SNV + raw\n",
    "  params = {'max_depth': [10], 'max_features': [30], 'n_estimators': [50]} \n",
    "\n",
    "elif DATASET == 'balanced':\n",
    "  params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'snv_balanced':\n",
    "  params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [100]} \n",
    "  \n",
    "elif DATASET == 'my_balanced':\n",
    "  params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [300]} \n",
    "\n",
    "elif DATASET == 'FS_my_balanced':\n",
    "  params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [300]} \n",
    "\n",
    "elif DATASET == 'FSv2_my_balanced':\n",
    "  params = {'max_depth': [None], 'max_features': [30], 'n_estimators': [100]} \n",
    "    \n",
    "elif DATASET == 'snv_my_balanced':\n",
    "  params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [300]}\n",
    "\n",
    "elif DATASET == 'snv_FS_my_balanced':\n",
    "  params = {'max_depth': [None], 'max_features': [10], 'n_estimators': [10]} \n",
    "\n",
    "elif DATASET == 'augmented':\n",
    "  params = {'max_depth': [None], 'max_features': [20], 'n_estimators': [100]} \n",
    "\n",
    "elif DATASET == 'augmented_FS':\n",
    "  params = {'max_depth': [None], 'max_features': [1], 'n_estimators': [50]} \n",
    "\n",
    "elif DATASET == 'smote':\n",
    "  params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [50]} \n",
    "\n",
    "elif DATASET == 'augmentedv3':\n",
    "  params = {'max_depth': [None], 'max_features': [10], 'n_estimators': [200]} \n",
    "\n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "  params = {'max_depth': [None], 'max_features': [10], 'n_estimators': [200]} \n",
    "    \n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "  params = {'max_depth': [None], 'max_features': [10], 'n_estimators': [300]} \n",
    "  \n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "  params = {'max_depth': [10], 'max_features': [1], 'n_estimators': [300]} \n",
    "\n",
    "elif DATASET == 'svmsmote':\n",
    "  params = {'max_depth': [None], 'max_features': [20], 'n_estimators': [50]} \n",
    "    \n",
    "elif DATASET == 'kmeanssmote':\n",
    "  params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [100]} \n",
    "  \n",
    "elif DATASET == 'adasynsmote':\n",
    "  params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [50]} \n",
    "    \n",
    "elif DATASET == 'bordersmote':\n",
    "  params = {'max_depth': [10], 'max_features': [5], 'n_estimators': [50]} \n",
    "    \n",
    "elif DATASET == 'snv_svmsmote':\n",
    "  params = {'max_depth': [None], 'max_features': [20], 'n_estimators': [50]} \n",
    "    \n",
    "elif DATASET == 'snv_FS_svmsmote':\n",
    "  params = {'max_depth': [None], 'max_features': [5], 'n_estimators': [100]} \n",
    "    \n",
    "grid_search  = GridSearchCV(clf_rf, params, scoring='accuracy', cv=10)\n",
    "grid_search.fit(training_data, np.ravel(training_labels))\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"params = {best_params} \\nbest_score: {best_score}\")\n",
    "best_rf = grid_search.best_estimator_\n",
    "gs_results['RF'] = {'accuracy':best_score, 'params':best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (James Brady, 2009)\n",
    "# import itertools\n",
    "# best_ensemble = {DATASET:{'accuracy':-1}}\n",
    "# models_dict = {'CART':best_cart,\n",
    "#                 'RF':best_rf,\n",
    "#                 'LR':best_lr, \n",
    "#                 'GNB': best_nb,\n",
    "#                 'kNN': best_knn, \n",
    "#                 'SVM-RBF': best_svmrbf, \n",
    "#                 'SVM-Lin': best_svmlin, \n",
    "#                 'SVM-Sig': best_svmsig, \n",
    "#                 'XGB': best_xgb, \n",
    "#                 'ADA': best_ada, \n",
    "#                 }\n",
    "# combinations = list(itertools.combinations(models_dict,3))\n",
    "# for combi in combinations:\n",
    "#     estimators = []\n",
    "#     # print(combi)\n",
    "#     for model in combi:\n",
    "#         # print(model)\n",
    "#         estimators.append((model, models_dict[model]))\n",
    "#     # create ensemble\n",
    "#     ensemble = VotingClassifier(\n",
    "#         estimators=estimators,\n",
    "#         voting='hard',\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "#     ensemble.fit(training_data, training_labels)\n",
    "#     accuracy = ensemble.score(testing_data, testing_labels)\n",
    "#     if accuracy > best_ensemble[DATASET]['accuracy']:\n",
    "#         best_ensemble[DATASET]['accuracy'] = accuracy\n",
    "#         best_ensemble[DATASET]['estimators'] = ensemble.estimators_\n",
    "#     print(combi, accuracy)\n",
    "\n",
    "# best_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8671328671328671"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensemble model (Sklearn, 2014), (Sklearn, 2023)\n",
    "if DATASET == 'snv_svmsmote':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('CART', best_cart), \n",
    "        ('RF', best_rf), \n",
    "        ('LR', best_lr), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-RBF', best_svmrbf)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'augmentedv3':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('RF', best_rf), \n",
    "        ('LR', best_lr), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-RBF', best_svmrbf), \n",
    "        ('ADA', best_ada)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'augmentedv3_FS':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('CART', best_cart), \n",
    "        ('RF', best_rf), \n",
    "        ('LR', best_lr), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-Sig', best_svmsig)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'snv_augmentedv3':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('RF', best_rf), \n",
    "        ('GNB', best_nb), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-RBF', best_svmrbf), \n",
    "        ('XGB', best_xgb)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'bordersmote':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('LR', best_lr), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-RBF', best_svmrbf), \n",
    "        ('SVM-Lin', best_svmlin), \n",
    "        ('XGB', best_xgb)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'augmented_FS':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('CART', best_cart), \n",
    "        ('LR', best_lr), \n",
    "        ('GNB', best_nb), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-RBF', best_svmrbf)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'kmeanssmote':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('RF', best_rf), \n",
    "        ('LR', best_lr), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-RBF', best_svmrbf), \n",
    "        ('XGB', best_xgb)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'svmsmote':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('CART', best_cart), \n",
    "        ('RF', best_rf), \n",
    "        ('GNB', best_nb), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-Lin', best_svmlin)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'smote':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('RF', best_rf), \n",
    "        ('LR', best_lr), \n",
    "        ('GNB', best_nb), \n",
    "        ('kNN', best_knn), \n",
    "        ('ADA', best_ada)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'adasynsmote':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('CART', best_cart), \n",
    "        ('RF', best_rf), \n",
    "        ('LR', best_lr), \n",
    "        ('kNN', best_knn), \n",
    "        ('XGB', best_xgb)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'snv_FS_balanced':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('RF', best_rf), \n",
    "        ('LR', best_lr), \n",
    "        ('GNB', best_nb), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-Lin', best_svmlin)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'raw':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('RF', best_rf), \n",
    "        ('GNB', best_nb), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-Lin', best_svmlin), \n",
    "        ('XGB', best_xgb)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "elif DATASET == 'feature_selectv2':\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('RF', best_rf), \n",
    "        ('LR', best_lr), \n",
    "        ('kNN', best_knn), \n",
    "        ('SVM-RBF', best_svmrbf), \n",
    "        ('SVM-Lin', best_svmlin)],\n",
    "        voting='hard',\n",
    "        n_jobs=-1)\n",
    "\n",
    "ensemble.fit(training_data, training_labels)\n",
    "accuracy = ensemble.score(testing_data, testing_labels)\n",
    "predictions = ensemble.transform(testing_data)\n",
    "gs_results['Ensemble'] = {'accuracy':accuracy}\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CART': {'accuracy': 0.8160000000000001, 'params': {'max_depth': None, 'max_features': 'log2'}}, 'GNB': {'accuracy': 0.6733333333333335, 'params': {'var_smoothing': 1e-20}}, 'kNN': {'accuracy': 0.8413333333333334, 'params': {'n_neighbors': 1}}, 'SVM-RBF': {'accuracy': 0.8773333333333333, 'params': {'C': 5000, 'gamma': 'scale'}}, 'SVM-Lin': {'accuracy': 0.8173333333333334, 'params': {'C': 1000, 'gamma': 'scale'}}, 'SVM-Sig': {'accuracy': 0.6266666666666667, 'params': {'C': 100, 'gamma': 'auto'}}, 'XGB': {'accuracy': 0.8480000000000001, 'params': {'max_depth': 3, 'n_estimators': 100}}, 'ADA': {'accuracy': 0.7133333333333333, 'params': {'learning_rate': 0.001, 'n_estimators': 1000}}, 'LR': {'accuracy': 0.796, 'params': {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}}, 'RF': {'accuracy': 0.8626666666666667, 'params': {'max_depth': None, 'max_features': 10, 'n_estimators': 200}}, 'Ensemble': {'accuracy': 0.8671328671328671}}\n",
      "dict_keys(['SVM-RBF', 'Ensemble', 'RF', 'XGB', 'kNN', 'SVM-Lin', 'CART', 'LR', 'ADA', 'GNB', 'SVM-Sig'])\n"
     ]
    }
   ],
   "source": [
    "# sorted GS models\n",
    "print(gs_results)\n",
    "gs_sorted_models = dict(sorted(gs_results.items(), key=lambda item: item[1]['accuracy'], reverse=True))\n",
    "print(gs_sorted_models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Waterhouse et al. (2021)) used to calculate class wise accuracy\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import numpy as np\n",
    " \n",
    "def compute_cfMatrix(y_pred, y_true, labels):\n",
    "    sensitivity = []\n",
    "    specificity = []\n",
    "    PPV = []\n",
    "    NPV = []\n",
    "    Accuracy = []\n",
    "    F1 = []\n",
    "    \n",
    "    perClass_cfM = multilabel_confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    # epsilon = 0.00001\n",
    "    for i in range (len(labels)):\n",
    "        TN = perClass_cfM[i][0,0]\n",
    "        FN = perClass_cfM[i][1,0]\n",
    "        TP = perClass_cfM[i][1,1]\n",
    "        FP = perClass_cfM[i][0,1]\n",
    "        # recall\n",
    "        r = TP/(TP+FN)\n",
    "        sensitivity.append(r)\n",
    "        # specificity\n",
    "        specificity.append(TN/(TN+FP))\n",
    "        # precision\n",
    "        p = TP/(TP+FP)\n",
    "        PPV.append(p)\n",
    "        # F1-score\n",
    "        F1.append(2*(p*r)/(p+r))\n",
    "        # NPV\n",
    "        NPV.append(TN/(TN+FN))\n",
    "        \n",
    "        Accuracy.append((TP+TN)/(TP+TN+FP+FN))\n",
    "\n",
    "    return sensitivity, specificity, PPV, NPV, Accuracy, F1, perClass_cfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING OUTPUTS FROM SKLEARN VS IMPORTED FUNCTION\n",
    "# predicted = best_cart.predict(testing_data)\n",
    "# sensitivity, specificity, PPV, NPV, Accuracy, F1, cfm = compute_cfMatrix(testing_labels, predicted, labels=[0,1,2])\n",
    "# print(f\"recall: {sensitivity},  \\nAccuracy: {(Accuracy)}, \\nF1: {F1}\")\n",
    "# print(f\"sharib: {cfm}, \\n\\nsklearn: {cm}\")\n",
    "\n",
    "# accuracy\n",
    "# 1: 96.5\n",
    "# 2: 80.4\n",
    "# 3: 78.3 \n",
    "\n",
    "# recall\n",
    "# 1: 90.9\n",
    "# 2: 84.1\n",
    "# 3: 65.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't predict class probilities for SVM-RBF\n",
      "can't predict class probilities for SVM-Lin\n",
      "can't predict class probilities for SVM-Sig\n",
      "can't predict class probilities for Ensemble\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('metrics/augmentedv3_dataset/model_metrics_accuracy_ensemble.json',\n",
       " 'metrics/augmentedv3_dataset/model_metrics_recall_ensemble.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate models (Sklearn, 2023)\n",
    "# model_metrics = {'accuracy', 'recall', 'precision', 'F1-score', 'ROC-AUC'}\n",
    "model_metrics = {}\n",
    "\n",
    "# all of the models\n",
    "models = [best_cart, best_rf, best_lr, best_nb, best_knn, best_svmrbf, best_svmlin, best_svmsig, best_xgb, best_ada, ensemble]\n",
    "model_names = ['CART', 'RF', 'LR', 'GNB', 'kNN', 'SVM-RBF', 'SVM-Lin', 'SVM-Sig', 'XGB', 'ADA', 'Ensemble']\n",
    "i=0\n",
    "for model in models:\n",
    "    # train on test set\n",
    "    predicted = model.predict(testing_data)\n",
    "    # generate cm against test labels\n",
    "    cm = confusion_matrix(testing_labels, predicted)\n",
    "    # print(cm)\n",
    "    accuracy = accuracy_score(testing_labels, predicted)\n",
    "    a, b, c, d, classwise_accuracy, f, g = compute_cfMatrix(testing_labels, predicted, labels=[0,1,2])\n",
    "    recall = recall_score(testing_labels, predicted, average=None)\n",
    "    precision = precision_score(testing_labels, predicted, average=None)\n",
    "    f1 = f1_score(testing_labels, predicted, average=None)\n",
    "\n",
    "    try:\n",
    "        predicted_prob = model.predict_proba(testing_data)\n",
    "        roc = roc_auc_score(testing_labels, predicted_prob, average=None, multi_class='ovr') \n",
    "        # print(accuracy, recall, precision, f1, roc)\n",
    "        model_metrics[model_names[i]] = {\n",
    "                                            'accuracy':accuracy, \n",
    "                                            'cw_accuracy': {\n",
    "                                                1:classwise_accuracy[0],\n",
    "                                                2:classwise_accuracy[1],\n",
    "                                                3:classwise_accuracy[2],\n",
    "                                            },\n",
    "                                            'recall':{\n",
    "                                                1:recall[0], \n",
    "                                                2:recall[1], \n",
    "                                                3:recall[2]\n",
    "                                            },\n",
    "                                            'precision':{\n",
    "                                                1:precision[0], \n",
    "                                                2:precision[1], \n",
    "                                                3:precision[2]\n",
    "                                            },\n",
    "                                            'f1_score':{\n",
    "                                                1:f1[0], \n",
    "                                                2:f1[1], \n",
    "                                                3:f1[2]\n",
    "                                            },\n",
    "                                            'ROC-AUC':{\n",
    "                                                1:roc[0], \n",
    "                                                2:roc[1], \n",
    "                                                3:roc[2]\n",
    "                                            }\n",
    "        }\n",
    "    except:\n",
    "        print(f\"can't predict class probilities for {model_names[i]}\")\n",
    "        # print(accuracy, recall, precision, f1)\n",
    "        model_metrics[model_names[i]] = {\n",
    "                                            'accuracy':accuracy, \n",
    "                                            'cw_accuracy': {\n",
    "                                                1:classwise_accuracy[0],\n",
    "                                                2:classwise_accuracy[1],\n",
    "                                                3:classwise_accuracy[2],\n",
    "                                            },\n",
    "                                            'recall':{\n",
    "                                                1:recall[0], \n",
    "                                                2:recall[1], \n",
    "                                                3:recall[2]\n",
    "                                            },\n",
    "                                            'precision':{\n",
    "                                                1:precision[0], \n",
    "                                                2:precision[1], \n",
    "                                                3:precision[2]\n",
    "                                            },\n",
    "                                            'f1_score':{\n",
    "                                                1:f1[0], \n",
    "                                                2:f1[1], \n",
    "                                                3:f1[2]\n",
    "                                            }\n",
    "        }\n",
    "    i+=1\n",
    "\n",
    "# (Gern Blanston, 2009)- sort by neoplasia recall\n",
    "sorted_metrics = dict(sorted(model_metrics.items(), key=lambda item: item[1]['recall'][3], reverse=True))\n",
    "# (holys, 2013)\n",
    "with open(FILENAME_RECALL, 'w') as fp:\n",
    "    json.dump(sorted_metrics, fp)\n",
    "\n",
    "# redo but sort by accuracy\n",
    "# (Gern Blanston, 2009)\n",
    "sorted_metrics_acc = dict(sorted(model_metrics.items(), key=lambda item: item[1]['accuracy'], reverse=True))\n",
    "# (holys, 2013)\n",
    "with open(FILENAME_ACC, 'w') as fp:\n",
    "    json.dump(sorted_metrics_acc, fp)\n",
    "model_metrics\n",
    "sorted_metrics\n",
    "FILENAME_ACC, FILENAME_RECALL\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs_sorted_models (acc): \n",
      "dict_keys(['SVM-RBF', 'Ensemble', 'RF', 'XGB', 'kNN', 'SVM-Lin', 'CART', 'LR', 'ADA', 'GNB', 'SVM-Sig'])\n",
      "\n",
      "sorted models (acc): \n",
      "dict_keys(['RF', 'Ensemble', 'SVM-RBF', 'kNN', 'XGB', 'CART', 'LR', 'SVM-Lin', 'ADA', 'SVM-Sig', 'GNB'])\n",
      "\n",
      "sorted models (recall): \n",
      "dict_keys(['ADA', 'Ensemble', 'RF', 'kNN', 'GNB', 'SVM-RBF', 'SVM-Sig', 'XGB', 'CART', 'LR', 'SVM-Lin'])\n"
     ]
    }
   ],
   "source": [
    "# print highest acc models from gridsearch\n",
    "print(f\"gs_sorted_models (acc): \\n{gs_sorted_models.keys()}\\n\")\n",
    "\n",
    "# highest acc models from test set\n",
    "print(f\"sorted models (acc): \\n{sorted_metrics_acc.keys()}\\n\")\n",
    "\n",
    "# highest recall from test set\n",
    "print(f\"sorted models (recall): \\n{sorted_metrics.keys()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'augmentedv3'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.3006993006993 7.982758620689656 11\n",
      "4.874125874125874 4.448275862068965 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'all': {'accuracy': 0.7546090273363, 'neoplasia recall': 0.7257053291536051},\n",
       " 'top6': {'accuracy': 0.8123543123543123, 'recall': 0.7413793103448275}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Neekhara, 2019)\n",
    "import json\n",
    "# DATASET = 'snv_svmsmote'\n",
    "\n",
    "# function to add to JSON\n",
    "def write_json(new_data, filename='metrics/scoreboard.json'):\n",
    "    with open(filename,'r+') as file:\n",
    "        # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[DATASET] = (new_data)\n",
    "        file_data = dict(sorted(file_data.items(), key=lambda item: item[1]['all']['accuracy'], reverse=True))\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 4)\n",
    "\n",
    "# calculate avg accuracy and recall\n",
    "accuracy = 0\n",
    "recall = 0\n",
    "count = 0\n",
    "for model in model_names:\n",
    "    # print(model_metrics[model]['accuracy'])\n",
    "    accuracy += model_metrics[model]['accuracy']\n",
    "    recall += model_metrics[model]['recall'][3]\n",
    "    count +=1\n",
    "\n",
    "t6acc = 0\n",
    "t6rec = 0\n",
    "count2 = 0\n",
    "for key in sorted_metrics_acc:\n",
    "    if count2 ==6:\n",
    "        break\n",
    "    t6acc += sorted_metrics_acc[key]['accuracy']\n",
    "    t6rec += sorted_metrics_acc[key]['recall'][3]\n",
    "    count2 +=1\n",
    "\n",
    "avg = {'all' : {'accuracy': accuracy/count, 'neoplasia recall': recall/count},\n",
    "       'top6' : {'accuracy': t6acc/count2, 'recall':t6rec/count2}}\n",
    "print(accuracy, recall, count)\n",
    "print(t6acc, t6rec, count2)\n",
    "if DATASET != 'test':\n",
    "    write_json(avg)\n",
    "else:\n",
    "    print(DATASET)\n",
    "avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013409000000004332\n"
     ]
    }
   ],
   "source": [
    "# time to predict a sample's class\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "ensemble.predict(testing_data)\n",
    "\n",
    "end = timeit.default_timer()\n",
    "duration = end-start\n",
    "\n",
    "# avg time to run each sample\n",
    "print(duration/len(testing_data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# observations on raw dataset\n",
    "XGBoost and ADAboost seem to have really overfit, because they severely underperform on unseen test data, compared to the accuracies they were achieving with gridsearch. IGNORE THIS: it is just because the test labels were not normalised!\n",
    "\n",
    "Although GNB has higher recall for neoplasia than kNN, kNN seems to be the best classifier overall. While GNB has highest recall for neoplasia, has 3rd lowest accuracy.\n",
    "\n",
    "Top models based on accuracy, from gridsearch, were RF, kNN, XGB, SVM-RBF, CART. Top models based on accuracy, from test set, were RF, kNN, SVM-RBF, CART, SVM-Lin. Therefore, RF, kNN, SVM-RBF, CART seem to perform well, in terms of accuracy, and don't seem to produce drastically different results with the test set, suggesting there isn't much overfitting\n",
    "\n",
    "gs_sorted_models (acc): \n",
    "(['RF', 'kNN', 'XGB', 'SVM-RBF', 'CART', 'LR', 'SVM-Lin', 'ADA', 'SVM-Sig', 'GNB'])\n",
    "\n",
    "sorted models (acc): \n",
    "(['RF', 'XGB', 'kNN', 'SVM-RBF', 'CART', 'SVM-Lin', 'LR', 'SVM-Sig', 'GNB', 'ADA'])\n",
    "\n",
    "sorted models (recall): \n",
    "(['GNB', 'kNN', 'CART', 'RF', 'SVM-Lin', 'XGB', 'LR', 'SVM-RBF', 'SVM-Sig', 'ADA'])\n",
    "\n",
    "# observations on feature selected dataset\n",
    "Some models decreased in performance, some increased, with largest increase being 6% increase in accuracy for SVM-Lin model. But overall, not worth, since the max accuracy of any of the models was lower than without feature selection. Maybe better feature selection is needed - an analytical solution rather than eyeball\n",
    "\n",
    "gs_sorted_models (acc): \n",
    "(['RF', 'SVM-RBF', 'kNN', 'XGB', 'CART', 'SVM-Lin', 'LR', 'ADA', 'GNB', 'SVM-Sig'])\n",
    "\n",
    "sorted models (acc): \n",
    "(['RF', 'XGB', 'kNN', 'SVM-RBF', 'CART', 'LR', 'SVM-Lin', 'GNB', 'ADA', 'SVM-Sig'])\n",
    "\n",
    "sorted models (recall): \n",
    "(['GNB', 'SVM-RBF', 'XGB', 'RF', 'kNN', 'SVM-Lin', 'CART', 'LR', 'SVM-Sig', 'ADA'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69968d942e1dcc7d0770d34dbcb974701730c09224194d51fbd302d9296a213d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
